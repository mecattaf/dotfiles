encryption: age
age:
  identity: "{{ .chezmoi.homeDir }}/.config/chezmoi/key.txt"
  recipient: "age1YOUR_PUBLIC_KEY_HERE"  # Replace with your key from Step 2

data:
  # ═══════════════════════════════════════════════════════════════════════════
  # 👤 USER INFORMATION
  # ═══════════════════════════════════════════════════════════════════════════
  user:
    name: "mecattaf"
    email: "thomas@mecattaf.dev"
  
  system:
    hostname: "{{ .chezmoi.hostname }}"
    os: "{{ .chezmoi.os }}"
    timezone: "America/New_York"
  
  # ═══════════════════════════════════════════════════════════════════════════
  # 🌐 TAILSCALE CONFIGURATION
  # ═══════════════════════════════════════════════════════════════════════════
  tailscale:
    hostname: "blueprint"
    tag: "tag:blueprint"
    tailnet: "tail8dd1.ts.net"
    full_hostname: "blueprint.tail8dd1.ts.net"
    oauth_client_id: "{{ .secrets.tailscale.oauth_client_id }}"
    oauth_client_secret: "{{ .secrets.tailscale.oauth_client_secret }}"


  
  # ═══════════════════════════════════════════════════════════════════════════
  # 🌉 INFRASTRUCTURE SERVICES REGISTRY
  # ═══════════════════════════════════════════════════════════════════════════
  infrastructure:
    network:
      name: "llm"
      subnet: "10.89.0.0/24"
      gateway: "10.89.0.1"
    
    services:
      # ─────────────────────────────────────────────────────────────────────────
      # Core Infrastructure Databases (Isolated per service)
      # ─────────────────────────────────────────────────────────────────────────
      litellm_postgres:
        hostname: "litellm-postgres"
        container_name: "litellm-postgres"
        port: 5432
        published_port: null  # Internal only
        image: "docker.io/pgvector/pgvector:pg16"
        db_name: "litellm"
        db_user: "litellm"
        volume: "litellm-postgres.volume"
        enabled: true
      
      litellm_redis:
        hostname: "litellm-redis"
        container_name: "litellm-redis"
        port: 6379
        published_port: null  # Internal only
        image: "docker.io/redis:latest"
        volume: "litellm-redis.volume"
        enabled: true
        requires: []
      
      openwebui_postgres:
        hostname: "openwebui-postgres"
        container_name: "openwebui-postgres"
        port: 5432
        published_port: null  # Internal only
        image: "docker.io/pgvector/pgvector:pg16"
        db_name: "openwebui"
        db_user: "openwebui"
        volume: "openwebui-postgres.volume"
        enabled: true
      
      openwebui_redis:
        hostname: "openwebui-redis"
        container_name: "openwebui-redis"
        port: 6379
        published_port: null  # Internal only
        image: "docker.io/redis:latest"
        volume: "openwebui-redis.volume"
        enabled: true
      
      searxng_redis:
        hostname: "searxng-redis"
        container_name: "searxng-redis"
        port: 6379
        published_port: null  # Internal only
        image: "docker.io/redis:latest"
        volume: "searxng-redis.volume"
        enabled_by: ["openwebui.providers.web_search_engine == 'searxng'"]
      
      # ─────────────────────────────────────────────────────────────────────────
      # LLM Services
      # ─────────────────────────────────────────────────────────────────────────
      litellm:
        hostname: "litellm"
        container_name: "litellm"
        port: 4000
        published_port: 4000
        image: "ghcr.io/berriai/litellm:main-stable"
        external_subdomain: "llm"
        requires: ["litellm_postgres", "litellm_redis"]
        enabled: true
        websocket: false
        description: "LLM Proxy Service"
        config_file: "litellm/litellm.yaml"
      
      openwebui:
        hostname: "openwebui"
        container_name: "openwebui"
        port: 8080
        published_port: 3000
        image: "ghcr.io/open-webui/open-webui:main"
        external_subdomain: "ai"
        requires: ["openwebui_postgres", "openwebui_redis", "litellm"]
        enabled: true
        websocket: true
        description: "Open WebUI Chat Interface"
        env_file: "openwebui/openwebui.env"
        volume: "openwebui.volume"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Document Processing (conditional based on content_extraction provider)
      # ─────────────────────────────────────────────────────────────────────────
      tika:
        hostname: "tika"
        container_name: "tika"
        port: 9998
        published_port: null  # Internal only
        image: "docker.io/apache/tika:latest-full"
        enabled_by: ["openwebui.providers.content_extraction == 'tika'"]
        websocket: false
        description: "Apache Tika Content Extraction"
        heap_size: "8g"  # Options: "1g", "2g", "4g", "8g"
      
      docling:
        hostname: "docling"
        container_name: "docling"
        port: 5001
        published_port: 5001
        image: "quay.io/docling-project/docling-serve:latest"
        external_subdomain: "docling"
        enabled_by: ["openwebui.providers.content_extraction == 'docling'"]
        websocket: false
        description: "Docling Document Processing"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Search Services (conditional based on web_search_engine provider)
      # ─────────────────────────────────────────────────────────────────────────
      searxng:
        hostname: "searxng"
        container_name: "searxng"
        port: 8080
        published_port: 8888
        image: "docker.io/searxng/searxng:latest"
        external_subdomain: "search"
        enabled_by: ["openwebui.providers.web_search_engine == 'searxng'"]
        requires: ["searxng_redis"]
        websocket: false
        description: "SearXNG Meta Search Engine"
        volume: "searxng.volume"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Code Execution (conditional based on code_execution_engine provider)
      # ─────────────────────────────────────────────────────────────────────────
      jupyter:
        hostname: "jupyter"
        container_name: "jupyter"
        port: 8888
        published_port: 8889
        image: "quay.io/jupyter/scipy-notebook:latest"
        external_subdomain: "jupyter"
        enabled_by: ["openwebui.providers.code_execution_engine == 'jupyter'"]
        websocket: true
        description: "Jupyter Code Interpreter"
        volume: "jupyter.volume"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Vector Databases (conditional based on vector_db provider)
      # ─────────────────────────────────────────────────────────────────────────
      qdrant:
        hostname: "qdrant"
        container_name: "qdrant"
        port: 6333
        grpc_port: 6334
        published_port: 6333  # Dashboard enabled
        image: "docker.io/qdrant/qdrant:latest"
        external_subdomain: "qdrant"
        enabled_by: ["openwebui.providers.vector_db == 'qdrant'"]
        websocket: false
        description: "Qdrant Vector Database"
        volume: "qdrant.volume"
      
      # Note: pgvector uses openwebui_postgres (no separate service)
      # Note: chroma embedded in OpenWebUI (no separate service)
      
      # ─────────────────────────────────────────────────────────────────────────
      # MCP Tool Server
      # ─────────────────────────────────────────────────────────────────────────
      mcp:
        hostname: "mcp"
        container_name: "mcp"
        port: 8000
        published_port: null  # Internal only
        image: "ghcr.io/open-webui/mcpo:latest"
        enabled: true
        websocket: false
        description: "Model Context Protocol Server"
        config_file: "mcp/config.json"

      # ─────────────────────────────────────────────────────────────────────────
      # Local AI Inference
      # ─────────────────────────────────────────────────────────────────────────
      llama_swap:
        hostname: "llama-swap"
        container_name: "llama-swap"
        port: 8080                         # Internal container port
        published_port: 9292               # Published to host
        bind: "127.0.0.1"
        image: "ghcr.io/mostlygeek/llama-swap:cpu"
        external_subdomain: "llama-swap"
        websocket: true
        enabled: true
        volume: "llama-swap.volume"
        description: "Local LLM Model Router"


      whisper:
        hostname: "whisper"
        container_name: "whisper"
        port: 8000                          # Internal container port
        published_port: 8765                # Published to host (no conflicts)
        bind: "127.0.0.1"
        image: "docker.io/fedirz/faster-whisper-server:latest-cpu"
        external_subdomain: "whisper"       # Optional: external access via Caddy
        enabled_by: ["openwebui.features.speech_to_text"]  # Auto-enable when STT enabled
        websocket: false
        description: "Whisper Speech-to-Text Service"
        volume: "whisper-cache.volume"



      edgetts:
        hostname: "edgetts"
        container_name: "edgetts"
        port: 5050                          # Internal container port
        published_port: 5050                # Published to host
        bind: "127.0.0.1"
        image: "ghcr.io/traefik/parakeet:latest"  # Edge-TTS compatible server
        external_subdomain: "tts"           # Optional: external access via Caddy
        enabled_by: ["openwebui.features.text_to_speech"]
        websocket: false
        description: "Edge-TTS Text-to-Speech Service"
        volume: "edgetts-cache.volume"
      
      # ─────────────────────────────────────────────────────────────────────────
      # System Management
      # ─────────────────────────────────────────────────────────────────────────
      cockpit:
        hostname: "{{ .tailscale.hostname }}"  # Runs on host
        container_name: "cockpit-ws"
        port: 9090
        published_port: 9090
        bind: "127.0.0.1"
        image: "quay.io/cockpit/ws:latest"
        external_subdomain: ""  # Root domain
        enabled: true
        websocket: true
        description: "System Management Dashboard"

  # ═══════════════════════════════════════════════════════════════════════════
  # 🔐 SECRETS REFERENCES (from encrypted_private_secrets.yaml)
  # ═══════════════════════════════════════════════════════════════════════════
  secrets:
    # Database passwords (one per service)
    databases:
      litellm: "{{ .secrets.databases.litellm }}"
      openwebui: "{{ .secrets.databases.openwebui }}"
    
    # Self-generated keys
    api_keys:
      litellm_master: "{{ .secrets.api_keys.litellm_master }}"
      openwebui_secret: "{{ .secrets.api_keys.openwebui_secret }}"
      jupyter_token: "{{ .secrets.api_keys.jupyter_token }}"
    
    # Cloud LLM providers
    llm_providers:
      openai: "{{ .secrets.llm_providers.openai }}"
      anthropic: "{{ .secrets.llm_providers.anthropic }}"
      gemini: "{{ .secrets.llm_providers.gemini }}"
    
    # Optional: Search providers
    search_providers:
      tavily: "{{ .secrets.search_providers.tavily }}"
      brave: "{{ .secrets.search_providers.brave }}"
    
    # Optional: Document processing
    document_providers:
      mistral_ocr: "{{ .secrets.document_providers.mistral_ocr }}"
    
    # Optional: Audio providers
    audio_providers:
      elevenlabs: "{{ .secrets.audio_providers.elevenlabs }}"
      deepgram: "{{ .secrets.audio_providers.deepgram }}"


  # ═══════════════════════════════════════════════════════════════════════════
  # 🚀 LITELLM - LLM Proxy Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  litellm:
    # Database connection (auto-generated from infrastructure.services)
    database_url: "postgresql://litellm:{{ .secrets.databases.litellm }}@{{ .infrastructure.services.litellm_postgres.hostname }}:{{ .infrastructure.services.litellm_postgres.port }}/litellm"
    
    # Redis cache connection (auto-generated)
    redis_url: "redis://{{ .infrastructure.services.litellm_redis.hostname }}:{{ .infrastructure.services.litellm_redis.port }}/0"
    
    # Master key for authentication
    master_key: "{{ .secrets.api_keys.litellm_master }}"
    
    # Models to expose
    models:
      - name: "gpt-4o"
        provider: "openai"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: true
      
      - name: "gpt-4o-mini"
        provider: "openai"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: true
      
      - name: "claude-3-5-sonnet"
        provider: "anthropic"
        api_key: "{{ .secrets.llm_providers.anthropic }}"
        enabled: true
      
      - name: "claude-3-5-haiku"
        provider: "anthropic"
        api_key: "{{ .secrets.llm_providers.anthropic }}"
        enabled: true
      
      - name: "gemini-2.0-flash"
        provider: "gemini"
        api_key: "{{ .secrets.llm_providers.gemini }}"
        enabled: true

      # ───────────────────────────────────────────────────────────────────────
      # LOCAL MODELS (via llama-swap → ramalama)
      # ───────────────────────────────────────────────────────────────────────
      - name: "gpt-oss-20b"
        provider: "llama-swap"
        enabled: true
        endpoint: "http://{{ .infrastructure.services.llama_swap.hostname }}:{{ .infrastructure.services.llama_swap.port }}"
        description: "OpenAI GPT-OSS 20B - Fast local model"
      
      - name: "gpt-oss-120b"
        provider: "llama-swap"
        enabled: true
        endpoint: "http://{{ .infrastructure.services.llama_swap.hostname }}:{{ .infrastructure.services.llama_swap.port }}"
        description: "OpenAI GPT-OSS 120B - Powerful local model"
    
    # General settings
    drop_params: true  # Essential for compatibility
    log_level: "INFO"

  # ═══════════════════════════════════════════════════════════════════════════
  # 🤖 LOCAL INFERENCE - RamaLama + llama-swap Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  local_inference:
    enabled: true
    
    # Default settings for all local models
    defaults:
      runtime: "llama.cpp"
      backend: "vulkan"              # AMD GPU acceleration
      ctx_size: 8192                 # Context window
      ngl: 999                       # GPU layers (all)
      temp: 0.7                      # Temperature
      auto_unload: true              # Enable TTL-based unloading
    
    # Model groups
    groups:
      heavy:
        swap: true
        description: "Primary large models (one active at a time)"
        members:
          - "gpt-oss-20b"
          - "gpt-oss-120b"
      
      task:
        swap: false
        description: "Always-loaded task models (run in parallel, never swap)"
        members:
          - "qwen3-0.6b"
          - "qwen3-4b"

      embeddings:
        swap: false
        description: "Embedding models for RAG (always loaded, parallel execution)"
        members:
          - "qwen3-embedding-8b"

    # Model definitions
    models:
      gpt_oss_20b:
        name: "gpt-oss-20b"
        display_name: "GPT-OSS 20B"
        description: "OpenAI's 20B parameter model - Fast general tasks"
        hf_repo: "openai/gpt-oss-20b-GGUF"
        hf_file: "gpt-oss-20b-q4_k_m.gguf"
        model_uri: "huggingface://openai/gpt-oss-20b-GGUF/gpt-oss-20b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 16
        context_length: 8192
        ctx_size: 8192
        ttl: 600
        group: "heavy"
        vulkan_driver: "RADV"
        flash_attn: true
        aliases:
          - "gpt-oss"
        enabled: true
      
      llama_4_scout_17b:
        name: "llama-4-scout-17b"
        display_name: "Llama 4 Scout 17B"
        description: "Extended context specialist - 10M token capability"
        model_uri: "huggingface://unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 58
        context_length: 10485760      # 10M tokens!
        ctx_size: 65536                # Practical limit for speed
        ttl: 900
        vulkan_driver: "AMDVLK"        # Better for long prompts
        flash_attn: true
        # Note: Can theoretically do 1M context in 128GB, but extremely slow
        # Practical: 65k context is much more usable
        enabled: false                 # Enable when you need long context
      
      qwen3_coder_30b:
        name: "qwen3-coder-30b"
        display_name: "Qwen3 Coder 30B"
        description: "Advanced code model - great for complex refactoring"
        model_uri: "huggingface://unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-BF16.gguf"
        quantization: "BF16"
        ram_required_gb: 60
        context_length: 8192
        ctx_size: 8192
        ttl: 900
        vulkan_driver: "RADV"
        flash_attn: true
        enabled: false                 # Enable when you need serious coding help
      
      qwen3_thinking_30b:
        name: "qwen3-thinking-30b"
        display_name: "Qwen3 Thinking 30B"
        description: "Deep reasoning specialist - thinks through problems"
        model_uri: "huggingface://unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF/Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 20
        context_length: 8192
        ctx_size: 8192
        ttl: 900
        vulkan_driver: "AMDVLK"        # Better for long reasoning prompts
        flash_attn: true
        aliases:
          - "thinking"
          - "qwen-thinking"
        enabled: false
      
      qwq_32b:
        name: "qwq-32b"
        display_name: "QwQ 32B"
        description: "Question-answering specialist with reasoning"
        model_uri: "huggingface://bartowski/Qwen_QwQ-32B-GGUF/Qwen_QwQ-32B-Q4_K_M.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 20
        context_length: 32768
        ctx_size: 32768
        ttl: 900
        vulkan_driver: "AMDVLK"
        flash_attn: true
        enabled: false
      
      gpt_oss_120b:
        name: "gpt-oss-120b"
        display_name: "GPT-OSS 120B"
        description: "OpenAI's 120B parameter model - Complex reasoning"
        hf_repo: "openai/gpt-oss-120b-GGUF"
        hf_file: "gpt-oss-120b-q4_k_m.gguf"
        model_uri: "huggingface://openai/gpt-oss-120b-GGUF/gpt-oss-120b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 80
        context_length: 8192
        ctx_size: 8192
        ttl: 900
        group: "heavy"
        vulkan_driver: "AMDVLK"        # Better for long prompts
        flash_attn: true
        enabled: true
      
      qwen3_235b:
        name: "qwen3-235b"
        display_name: "Qwen3 235B"
        description: "Flagship model - incredible capability, fits in 128GB"
        model_uri: "huggingface://unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf"
        quantization: "Q3_K_XL"
        ram_required_gb: 97
        context_length: 131072
        ctx_size: 65536                # Conservative - can do 130k max
        ttl: 1800                      # 30 minutes
        vulkan_driver: "AMDVLK"        # Better for complex prompts
        flash_attn: true
        batch_size: 2048
        # Note: This is HUGE but fits in 128GB with Q3_K quantization
        # Expect slower inference but incredible quality
        enabled: false                 # Enable when you need the best

      qwen3_0_6b:
        name: "qwen3-0.6b"
        display_name: "Qwen3 0.6B"
        description: "Ultra-fast task model - title generation"
        model_uri: "huggingface://Qwen/Qwen3-0.6B-GGUF/qwen3-0.6b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 1
        context_length: 32768
        ctx_size: 4096
        ttl: 0  # Never unload
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-tiny"
        enabled: true
      
      qwen3_4b:
        name: "qwen3-4b"
        display_name: "Qwen3 4B"
        description: "Balanced task model - tags, queries, analysis"
        model_uri: "huggingface://Qwen/Qwen3-4B-GGUF/qwen3-4b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 3
        context_length: 32768
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-medium"
        enabled: true
      
      # Optional models for manual experimentation
      qwen3_1_7b:
        name: "qwen3-1.7b"
        display_name: "Qwen3 1.7B"
        description: "Fast task model - metadata tagging, summarization"
        model_uri: "huggingface://Qwen/Qwen3-1.7B-GGUF/qwen3-1.7b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 2
        context_length: 32768
        ctx_size: 8192
        ttl: 600  # Auto-unload after 10 min
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-small"
        enabled: false  # Disabled by default
      
      qwen3_8b:
        name: "qwen3-8b"
        display_name: "Qwen3 8B"
        description: "High-quality task model - complex reasoning"
        model_uri: "huggingface://Qwen/Qwen3-8B-GGUF/qwen3-8b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 6
        context_length: 32768
        ctx_size: 8192
        ttl: 600  # Auto-unload after 10 min
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-large"
        enabled: false  # Disabled by default
      
      qwen3_14b:
        name: "qwen3-14b"
        display_name: "Qwen3 14B"
        description: "Premium task model - advanced reasoning"
        model_uri: "huggingface://Qwen/Qwen3-14B-GGUF/qwen3-14b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 10
        context_length: 32768
        ctx_size: 8192
        ttl: 600  # Auto-unload after 10 min
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-xlarge"
        enabled: false  # Disabled by default

      # ═══════════════════════════════════════════════════════════════════════
      # IBM GRANITE 4.0 MODELS
      # ═══════════════════════════════════════════════════════════════════════
      
      granite_4_0_h_micro:
        name: "granite-4.0-h-micro"
        display_name: "Granite 4.0 H-Micro"
        description: "IBM's 3B enterprise model - Fast, efficient for edge use"
        model_uri: "huggingface://unsloth/granite-4.0-h-micro-GGUF/granite-4.0-h-micro-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 3
        context_length: 131072
        ctx_size: 8192
        ttl: 600
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "granite-micro"
        enabled: false
        aliases:
          - "granite-micro"
          - "granite-3b"
      
      granite_4_0_h_small:
        name: "granite-4.0-h-small"
        display_name: "Granite 4.0 H-Small"
        description: "IBM's 32B MoE model (9B active) - Enterprise workhorse"
        model_uri: "huggingface://unsloth/granite-4.0-h-small-GGUF/granite-4.0-h-small-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 22
        context_length: 131072
        ctx_size: 16384
        ttl: 900
        group: "heavy"
        vulkan_driver: "AMDVLK"
        flash_attn: true
        shortname: "granite-small"
        enabled: false
        aliases:
          - "granite-small"
          - "granite-32b"

      # ═══════════════════════════════════════════════════════════════════════
      # EMBEDDING MODELS (for RAG/Vector Search)
      # ═══════════════════════════════════════════════════════════════════════
      
      qwen3_embedding_0_6b:
        name: "qwen3-embedding-0.6b"
        display_name: "Qwen3 Embedding 0.6B"
        description: "Ultra-fast embedding model - 0.6B parameters"
        model_uri: "huggingface://Qwen/Qwen3-Embedding-0.6B-GGUF/qwen3-embedding-0.6b-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 1
        context_length: 8192
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false  # Not applicable for embeddings
        embedding_dimension: 1024
        shortname: "qwen3-emb-tiny"
        enabled: false
      
      qwen3_embedding_4b:
        name: "qwen3-embedding-4b"
        display_name: "Qwen3 Embedding 4B"
        description: "Balanced embedding model - 4B parameters"
        model_uri: "huggingface://Qwen/Qwen3-Embedding-4B-GGUF/qwen3-embedding-4b-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 5
        context_length: 8192
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        embedding_dimension: 2048
        shortname: "qwen3-emb-medium"
        enabled: false
      
      qwen3_embedding_8b:
        name: "qwen3-embedding-8b"
        display_name: "Qwen3 Embedding 8B"
        description: "High-quality embedding model - 8B parameters (DEFAULT)"
        model_uri: "huggingface://Qwen/Qwen3-Embedding-8B-GGUF/qwen3-embedding-8b-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 9
        context_length: 8192
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        embedding_dimension: 4096
        shortname: "qwen3-emb-large"
        enabled: true  # ✅ DEFAULT ENABLED
        aliases:
          - "qwen-embed"
          - "default-embedding"

      nomic_embed_text_v1_5:
        name: "nomic-embed-text-v1.5"
        display_name: "Nomic Embed Text v1.5"
        description: "OpenAI-compatible embedding (text-embedding-3-small alternative)"
        model_uri: "huggingface://nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 1
        context_length: 8192
        ctx_size: 8192
        ttl: 0
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        enabled: false
        aliases:
          - "text-embedding-3-small"
      
      voyage_3_large:
        name: "voyage-3-large"
        display_name: "Voyage 3 Large"
        description: "High-capacity embedding - 10B parameters"
        model_uri: "huggingface://voyageai/voyage-3-large-GGUF/voyage-3-large-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 11
        context_length: 8192
        ctx_size: 8192
        ttl: 0
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        enabled: false
        aliases:
          - "text-embedding-3-large"

  # ═══════════════════════════════════════════════════════════════════════════
  # 🎤 local whisper section
  # ═══════════════════════════════════════════════════════════════════════════
  whisper:
    # Model cache directory - shared with host CLI tools
    cache_dir: "{{ .chezmoi.homeDir }}/.cache/huggingface"
    
    # Model selection
    # Options: tiny (75MB), base (142MB), small (466MB), medium (1.5GB), large-v3 (2.9GB)
    model: "large-v3"
    
    # Language configuration
    language: "en"             # Specific language, or "" for auto-detect (slower)
    
    # Inference settings
    beam_size: 5               # Higher = more accurate but slower (1-10)
    compute_type: "auto"       # auto, int8, float16, float32
    device: "auto"             # auto detects Vulkan GPU
    
    # Performance notes for your AMD Strix Halo:
    # - base model: ~16x real-time (6sec audio in 0.375sec)
    # - small model: ~6x real-time
    # - large-v3: ~1x real-time

  # ═══════════════════════════════════════════════════════════════════════════
  # 🔊 Edge-TTS Configuration (TTS)
  # Add after whisper section
  # ═══════════════════════════════════════════════════════════════════════════
  edgetts:
    # Audio format
    response_format: "mp3"     # Options: mp3, opus, aac, flac, wav, pcm
    
    # Speech speed
    speed: "1.0"               # 0.5 = half speed, 2.0 = double speed
    
    # Default voice
    # Full list: https://speech.microsoft.com/portal/voicegallery
    voice: "en-AU-NatashaNeural"
    
    # Alternative voices to try:
    # English (US): en-US-AriaNeural, en-US-GuyNeural, en-US-JennyNeural
    # English (UK): en-GB-SoniaNeural, en-GB-RyanNeural
    # English (AU): en-AU-NatashaNeural, en-AU-WilliamNeural
    
    # Logging
    log_level: "info"          # debug, info, warning, error
    
    # Available voices by model (OpenAI-compatible mapping)
    models:
      tts-1:
        alloy: "en-US-AriaNeural"
        echo: "en-US-GuyNeural"
        fable: "en-GB-RyanNeural"
        onyx: "en-US-DavisNeural"
        nova: "en-US-JennyNeural"
        shimmer: "en-US-AmberNeural"
      
      tts-1-hd:
        alloy: "en-US-AriaNeural"
        echo: "en-US-GuyNeural"
        fable: "en-GB-RyanNeural"
        onyx: "en-US-DavisNeural"
        nova: "en-US-JennyNeural"
        shimmer: "en-US-AmberNeural"
       
  # ═══════════════════════════════════════════════════════════════════════════
  # 🤖 OPENWEBUI - Feature-Centric Configuration (29 Decision Variables)
  # ═══════════════════════════════════════════════════════════════════════════
  openwebui:
    # ─────────────────────────────────────────────────────────────────────────
    # 🎯 LAYER 1: Feature Flags (What capabilities to enable?)
    # ─────────────────────────────────────────────────────────────────────────
    features:
      # Core AI capabilities
      rag: true                           # 🔑 Decision 1: Enable RAG?
      web_search: true                    # 🔑 Decision 2: Enable web search?
      image_generation: false             # 🔑 Decision 3: Enable image gen?
      speech_to_text: true               # 🔑 Decision 4: Enable STT?
      text_to_speech: true               # 🔑 Decision 5: Enable TTS?
      
      # Code execution
      code_execution: true                # 🔑 Decision 6: Enable code execution?
      code_interpreter: true              # 🔑 Decision 7: Enable code interpreter?
      
      # Cloud integrations
      google_drive: false                 # 🔑 Decision 8: Enable Google Drive?
      onedrive: false                     # 🔑 Decision 9: Enable OneDrive?
      
      # Authentication
      oauth_signup: false                 # 🔑 Decision 10: Enable OAuth?
      ldap: false                         # 🔑 Decision 11: Enable LDAP?
      
      # Advanced features
      title_generation: false             # 🔑 Decision 12: Auto-generate titles?
      autocomplete_generation: false      # 🔑 Decision 13: Enable autocomplete?
      tags_generation: false              # 🔑 Decision 14: Auto-generate tags?
      
      # Infrastructure
      websocket_support: true             # 🔑 Decision 15: Enable WebSocket?
      direct_connections: true            # 🔑 Decision 16: Direct model connections?
    
    # ─────────────────────────────────────────────────────────────────────────
    # 🔌 LAYER 2: Provider Selections (Which implementation?)
    # ─────────────────────────────────────────────────────────────────────────
    providers:
      # Vector database (controls 48+ variables)
      vector_db: "pgvector"               # 🔑 Decision 17: chroma | pgvector | qdrant | milvus | opensearch | elasticsearch | pinecone
      
      # RAG components (controls 16+ variables)
      rag_embedding: "openai"                   # 🔑 Decision 18: "" (local) | ollama | openai
      content_extraction: "tika"          # 🔑 Decision 19: tika | docling | mistral_ocr | document_intelligence | external
      text_splitter: "character"          # 🔑 Decision 20: character | token
      
      # Web search (controls 50+ variables)
      web_search_engine: "searxng"        # 🔑 Decision 21: searxng | tavily | brave | google_pse | duckduckgo | kagi | etc
      web_loader: "requests"              # 🔑 Decision 22: requests | playwright | safe_web
      
      # Image generation (controls 12+ variables)
      image_engine: "openai"              # 🔑 Decision 23: openai | comfyui | automatic1111 | gemini
      
      # Audio (controls 20+ variables)
      stt_engine: "openai"                      # 🔑 Decision 24: "" (local whisper) | openai | azure | deepgram
      tts_engine: "openai"                # 🔑 Decision 25: openai | azure | elevenlabs | transformers
      
      # Code execution (controls 8+ variables)
      code_execution_engine: "jupyter"    # 🔑 Decision 26: jupyter | pyodide
      code_interpreter_engine: "jupyter"  # 🔑 Decision 27: jupyter | pyodide
      
      # Storage (controls 12+ variables)
      storage_provider: ""                # 🔑 Decision 28: "" (local) | s3 | gcs | azure
      
      # Authentication (controls 10+ variables)
      auth_provider: "local"              # 🔑 Decision 29: local | oauth | ldap
    
    # ─────────────────────────────────────────────────────────────────────────
    # ⚙️  LAYER 3: Provider Configurations (How to configure?)
    # Only the selected provider's config is used at runtime
    # ─────────────────────────────────────────────────────────────────────────
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ VECTOR DATABASE CONFIGS (Decision 17)                               │
    # └─────────────────────────────────────────────────────────────────────┘
    vector_db_config:
      pgvector:
        # Auto-generated from infrastructure.services
        db_url: "postgresql://openwebui:{{ .secrets.databases.openwebui }}@{{ .infrastructure.services.openwebui_postgres.hostname }}:{{ .infrastructure.services.openwebui_postgres.port }}/openwebui"
        vector_dimension: 384
      
      chroma:
        # Embedded - no external service needed
        tenant: "default_tenant"
        database: "default_database"
      
      qdrant:
        # Auto-generated from infrastructure.services
        uri: "http://{{ .infrastructure.services.qdrant.hostname }}:{{ .infrastructure.services.qdrant.port }}"
        grpc_port: "{{ .infrastructure.services.qdrant.grpc_port }}"
        api_key: ""  # Optional
        prefer_grpc: false
        on_disk: true
      
      milvus:
        uri: "http://milvus:19530"
        token: ""
        index_type: "HNSW"
        metric_type: "COSINE"
      
      elasticsearch:
        url: "http://elasticsearch:9200"
        api_key: ""
        index_prefix: "openwebui"
      
      opensearch:
        uri: "http://opensearch:9200"
        username: "admin"
        password: ""
      
      pinecone:
        api_key: ""
        environment: "us-east-1"
        index_name: "openwebui"
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ RAG EMBEDDING CONFIGS (Decision 18)                                 │
    # └─────────────────────────────────────────────────────────────────────┘
    rag_embedding_config:
      local:
        model: "sentence-transformers/all-MiniLM-L6-v2"
        device: "cpu"
        batch_size: 1
      
      ollama:
        # Used when rag_embedding: "ollama" (native Ollama daemon)
        base_url: "http://ollama:11434"
        model: "nomic-embed-text:latest"

      openai:
        base_url: "http://{{ .infrastructure.services.litellm.hostname }}:{{ .infrastructure.services.litellm.port }}/v1"
        api_key: "{{ .secrets.api_keys.litellm_master }}"
        model: "qwen3-embedding-8b"  # ← DEFAULT embedding model

    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ CONTENT EXTRACTION CONFIGS (Decision 19)                            │
    # └─────────────────────────────────────────────────────────────────────┘
    content_extraction_config:
      tika:
        # Auto-generated from infrastructure.services
        server_url: "http://{{ .infrastructure.services.tika.hostname }}:{{ .infrastructure.services.tika.port }}"
      
      docling:
        # Auto-generated from infrastructure.services
        server_url: "http://{{ .infrastructure.services.docling.hostname }}:{{ .infrastructure.services.docling.port }}"
        ocr_engine: "tesseract"
        ocr_lang: "eng"
      
      mistral_ocr:
        api_key: "{{ .secrets.document_providers.mistral_ocr }}"
      
      external:
        url: ""
        api_key: ""
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ WEB SEARCH CONFIGS (Decision 21)                                    │
    # └─────────────────────────────────────────────────────────────────────┘
    web_search_config:
      searxng:
        # Auto-generated from infrastructure.services
        query_url: "http://{{ .infrastructure.services.searxng.hostname }}:{{ .infrastructure.services.searxng.port }}/search?q=<query>&format=json"
      
      tavily:
        api_key: "{{ .secrets.search_providers.tavily }}"
        extract_depth: "basic"
      
      brave:
        api_key: "{{ .secrets.search_providers.brave }}"
      
      google_pse:
        api_key: ""
        engine_id: ""
      
      duckduckgo: {}
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ WEB LOADER CONFIGS (Decision 22)                                    │
    # └─────────────────────────────────────────────────────────────────────┘
    web_loader_config:
      requests: {}
      
      playwright:
        ws_url: "ws://playwright:3000"
        timeout: 10000
      
      safe_web: {}
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ IMAGE GENERATION CONFIGS (Decision 23)                              │
    # └─────────────────────────────────────────────────────────────────────┘
    image_generation_config:
      openai:
        # Auto-generated - uses LiteLLM proxy
        base_url: "http://{{ .infrastructure.services.litellm.hostname }}:{{ .infrastructure.services.litellm.port }}/v1"
        api_key: "{{ .secrets.api_keys.litellm_master }}"
        model: "dall-e-3"
      
      comfyui:
        base_url: "http://comfyui:8188"
        workflow: ""
      
      automatic1111:
        base_url: "http://automatic1111:7860"
        api_auth: ""
      
      gemini:
        api_key: "{{ .secrets.llm_providers.gemini }}"
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ AUDIO CONFIGS (Decisions 24-25)                                     │
    # └─────────────────────────────────────────────────────────────────────┘
    audio_config:
      stt:
        openai:
        # Auto-generated - points to local whisper container
          base_url: "http://{{ .infrastructure.services.whisper.hostname }}:{{ .infrastructure.services.whisper.port }}/v1"
          api_key: "sk-whisper-local"  # Dummy key (not validated for local)
          model: "whisper-1"

        deepgram:
          api_key: "{{ .secrets.audio_providers.deepgram }}"
      
      tts:
        openai:
          # Auto-generated - points to edgetts container
          base_url: "http://{{ .infrastructure.services.edgetts.hostname }}:{{ .infrastructure.services.edgetts.port }}/v1"
          api_key: "sk-edgetts-local"
          model: "tts-1"
          voice: "{{ .edgetts.models.tts-1.alloy }}"  # Default to Aria
        
        elevenlabs:
          api_key: "{{ .secrets.audio_providers.elevenlabs }}"
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ CODE EXECUTION CONFIGS (Decisions 26-27)                            │
    # └─────────────────────────────────────────────────────────────────────┘
    code_execution_config:
      jupyter:
        # Auto-generated from infrastructure.services
        url: "http://{{ .infrastructure.services.jupyter.hostname }}:{{ .infrastructure.services.jupyter.port }}"
        auth: "token"
        token: "{{ .secrets.api_keys.jupyter_token }}"
        timeout: 60
      
      pyodide: {}
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ STORAGE CONFIGS (Decision 28)                                       │
    # └─────────────────────────────────────────────────────────────────────┘
    storage_config:
      local: {}
      
      s3:
        endpoint_url: ""
        bucket_name: "openwebui-storage"
        region_name: "us-east-1"
        access_key_id: ""
        secret_access_key: ""
      
      gcs:
        bucket_name: "openwebui-storage"
        credentials_json: ""
      
      azure:
        endpoint: ""
        container_name: "openwebui-storage"
        key: ""
    
    # ─────────────────────────────────────────────────────────────────────────
    # 🎨 General Settings (Provider-agnostic)
    # ─────────────────────────────────────────────────────────────────────────
    general:
      # Core connection (auto-generated from infrastructure.services)
      database_url: "postgresql://openwebui:{{ .secrets.databases.openwebui }}@{{ .infrastructure.services.openwebui_postgres.hostname }}:{{ .infrastructure.services.openwebui_postgres.port }}/openwebui"
      openai_api_base_url: "http://{{ .infrastructure.services.litellm.hostname }}:{{ .infrastructure.services.litellm.port }}/v1"
      openai_api_key: "{{ .secrets.api_keys.litellm_master }}"
      webui_secret_key: "{{ .secrets.api_keys.openwebui_secret }}"
      
      # Branding
      webui_name: "Leger AI"
      custom_name: "Blueprint LLM Stack"
      
      # Authentication
      webui_auth: false
      enable_signup: false
      default_user_role: "user"
      
      log_level: "info"  # debug | info | warning | error
      
      # Performance
      enable_realtime_chat_save: false
      
      # RAG settings
      rag_top_k: 3
      chunk_size: 1500
      chunk_overlap: 100
      pdf_extract_images: true
      
      # File upload limits
      rag_file_max_size: 50
      rag_file_max_count: 10
      
      enable_search_query: true
      enable_retrieval_query_generation: false
      
      # Web search settings
      web_search_result_count: 3
      web_search_concurrent_requests: 10
      
      enable_reranking: false
      reranking_model: "BAAI/bge-reranker-v2-m3"
      enable_hybrid_search: false
      enable_query_rewriting: false
      
      # ────────────────────────────────────────────────────────────────────────
      # 🤖 TASK MODEL ASSIGNMENTS (via LiteLLM)
      # ────────────────────────────────────────────────────────────────────────
      task_models:
        # Ultra-fast (0.6B) - single-task operations
        title_generation: "qwen3-0.6b"
        autocomplete_generation: "qwen3-0.6b"
        
        # Balanced (4B) - multi-step operations
        tags_generation: "qwen3-4b"
        query_generation: "qwen3-4b"
        search_query_generation: "qwen3-4b"
        rag_template_generation: "qwen3-4b"
    
    # ─────────────────────────────────────────────────────────────────────────
    # 🔧 Service Configuration
    # ─────────────────────────────────────────────────────────────────────────
    service:
      timeout_start_sec: 900
      restart: "on-failure"
      restart_sec: 10

  # ═══════════════════════════════════════════════════════════════════════════
  # 🎨 CADDY - Reverse Proxy (Auto-generated from infrastructure.services)
  # ═══════════════════════════════════════════════════════════════════════════
  caddy:
    # Caddy automatically configures routes based on infrastructure.services
    # where external_subdomain is defined
    auto_https: "off"  # Tailscale handles TLS
    
    # Optional: Manual route overrides
    custom_routes: []
  
  # ═══════════════════════════════════════════════════════════════════════════
  # 🔍 SEARXNG - Search Engine Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  searxng:
    # Redis connection (auto-generated from infrastructure.services)
    redis_url: "redis://{{ .infrastructure.services.searxng_redis.hostname }}:{{ .infrastructure.services.searxng_redis.port }}/0"
    
    # Base URL (auto-generated from Tailscale config)
    base_url: "https://{{ .infrastructure.services.searxng.external_subdomain }}.{{ .tailscale.full_hostname }}"
    
    # Settings
    safe_search: 0
    max_page: 5
    autocomplete: "duckduckgo"
    
    # Engines
    engines:
      - name: "duckduckgo"
        enabled: true
      - name: "google"
        enabled: true
      - name: "brave"
        enabled: true
      - name: "wikipedia"
        enabled: true
      - name: "github"
        enabled: true
