encryption: age
age:
  identity: "{{ .chezmoi.homeDir }}/.config/chezmoi/key.txt"
  recipient: "age1548mrmh00zlmd38c2xt37wu7zaszth4sgeyqfmljg55j0asj2evqgjp93f"

data:
  # ═══════════════════════════════════════════════════════════════════════════
  # 👤 USER INFORMATION
  # ═══════════════════════════════════════════════════════════════════════════
  user:
    name: "mecattaf"
    email: "thomas@mecattaf.dev"
  
  system:
    hostname: "{{ .chezmoi.hostname }}"
    os: "{{ .chezmoi.os }}"
    timezone: "America/New_York"
  
  # ═══════════════════════════════════════════════════════════════════════════
  # 🌐 TAILSCALE AND GITHUB CONFIGURATION
  # ═══════════════════════════════════════════════════════════════════════════
  tailscale:
    hostname: "blueprint"
    tag: "tag:blueprint"
    tailnet: "tail8dd1.ts.net"
    full_hostname: "blueprint.tail8dd1.ts.net"
    oauth_client_id: "{{ .secrets.tailscale.oauth_client_id }}"
    oauth_client_secret: "{{ .secrets.tailscale.oauth_client_secret }}"

  github:
    git_protocol: "https"
    user: "mecattaf"
    cli_pat: "{{ .secrets.github.cli_pat }}"

  # ═══════════════════════════════════════════════════════════════════════════
  # 🌉 INFRASTRUCTURE SERVICES REGISTRY
  # ═══════════════════════════════════════════════════════════════════════════
  infrastructure:
    network:
      name: "llm"
      subnet: "10.89.0.0/24"
      gateway: "10.89.0.1"
    
    services:
      # ─────────────────────────────────────────────────────────────────────────
      # Core Infrastructure Databases (Isolated per service)
      # ─────────────────────────────────────────────────────────────────────────
      litellm_postgres:
        hostname: "litellm-postgres"
        container_name: "litellm-postgres"
        port: 5432
        published_port: null  # Internal only
        image: "docker.io/pgvector/pgvector:pg16"
        db_name: "litellm"
        db_user: "litellm"
        volume: "litellm-postgres.volume"
        enabled: true
      
      litellm_redis:
        hostname: "litellm-redis"
        container_name: "litellm-redis"
        port: 6379
        published_port: null  # Internal only
        image: "docker.io/redis:latest"
        volume: "litellm-redis.volume"
        enabled: true
        requires: []
      
      openwebui_postgres:
        hostname: "openwebui-postgres"
        container_name: "openwebui-postgres"
        port: 5432
        published_port: null  # Internal only
        image: "docker.io/pgvector/pgvector:pg16"
        db_name: "openwebui"
        db_user: "openwebui"
        volume: "openwebui-postgres.volume"
        enabled: true
      
      openwebui_redis:
        hostname: "openwebui-redis"
        container_name: "openwebui-redis"
        port: 6379
        published_port: null  # Internal only
        image: "docker.io/redis:latest"
        volume: "openwebui-redis.volume"
        enabled: true
      
      searxng_redis:
        hostname: "searxng-redis"
        container_name: "searxng-redis"
        port: 6379
        published_port: null  # Internal only
        image: "docker.io/redis:latest"
        volume: "searxng-redis.volume"
        enabled_by: ["openwebui.providers.web_search_engine == 'searxng'"]
      
      # ─────────────────────────────────────────────────────────────────────────
      # LLM Services
      # ─────────────────────────────────────────────────────────────────────────
      litellm:
        hostname: "litellm"
        container_name: "litellm"
        port: 4000
        published_port: 4000
        image: "ghcr.io/berriai/litellm:main-stable"
        external_subdomain: "llm"
        requires: ["litellm_postgres", "litellm_redis"]
        enabled: true
        websocket: false
        description: "LLM Proxy Service"
        config_file: "litellm/litellm.yaml"
      
      openwebui:
        hostname: "openwebui"
        container_name: "openwebui"
        port: 8080
        published_port: 3000
        image: "ghcr.io/open-webui/open-webui:main"
        external_subdomain: "ai"
        requires: ["openwebui_postgres", "openwebui_redis", "litellm"]
        enabled: true
        websocket: true
        description: "Open WebUI Chat Interface"
        env_file: "openwebui/openwebui.env"
        volume: "openwebui.volume"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Document Processing (conditional based on content_extraction provider)
      # ─────────────────────────────────────────────────────────────────────────
      tika:
        hostname: "tika"
        container_name: "tika"
        port: 9998
        published_port: null  # Internal only
        image: "docker.io/apache/tika:latest-full"
        enabled_by: ["openwebui.providers.content_extraction == 'tika'"]
        websocket: false
        description: "Apache Tika Content Extraction"
        heap_size: "8g"  # Options: "1g", "2g", "4g", "8g"
      
      docling:
        hostname: "docling"
        container_name: "docling"
        port: 5001
        published_port: 5001
        image: "quay.io/docling-project/docling-serve:latest"
        external_subdomain: "docling"
        enabled_by: ["openwebui.providers.content_extraction == 'docling'"]
        websocket: false
        description: "Docling Document Processing"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Search Services (conditional based on web_search_engine provider)
      # ─────────────────────────────────────────────────────────────────────────
      searxng:
        hostname: "searxng"
        container_name: "searxng"
        port: 8080
        published_port: 8888
        image: "docker.io/searxng/searxng:latest"
        external_subdomain: "search"
        enabled_by: ["openwebui.providers.web_search_engine == 'searxng'"]
        requires: ["searxng_redis"]
        websocket: false
        description: "SearXNG Meta Search Engine"
        volume: "searxng.volume"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Code Execution (conditional based on code_execution_engine provider)
      # ─────────────────────────────────────────────────────────────────────────
      jupyter:
        hostname: "jupyter"
        container_name: "jupyter"
        port: 8888
        published_port: 8889
        image: "quay.io/jupyter/scipy-notebook:latest"
        external_subdomain: "jupyter"
        enabled_by: ["openwebui.providers.code_execution_engine == 'jupyter'"]
        websocket: true
        description: "Jupyter Code Interpreter"
        volume: "jupyter.volume"
      
      # ─────────────────────────────────────────────────────────────────────────
      # Vector Databases (conditional based on vector_db provider)
      # ─────────────────────────────────────────────────────────────────────────
      qdrant:
        hostname: "qdrant"
        container_name: "qdrant"
        port: 6333
        grpc_port: 6334
        published_port: 6333  # Dashboard enabled
        image: "docker.io/qdrant/qdrant:latest"
        external_subdomain: "qdrant"
        enabled_by: ["openwebui.providers.vector_db == 'qdrant'"]
        websocket: false
        description: "Qdrant Vector Database"
        volume: "qdrant.volume"
      
      # Note: pgvector uses openwebui_postgres (no separate service)
      # Note: chroma embedded in OpenWebUI (no separate service)
      
      # ─────────────────────────────────────────────────────────────────────────
      # MCP Tool Server
      # ─────────────────────────────────────────────────────────────────────────
      mcp:
        hostname: "mcp"
        container_name: "mcp"
        port: 8000
        published_port: null  # Internal only
        image: "ghcr.io/open-webui/mcpo:latest"
        enabled: true
        websocket: false
        description: "Model Context Protocol Server"
        config_file: "mcp/config.json"

      # ─────────────────────────────────────────────────────────────────────────
      # Local AI Inference
      # ─────────────────────────────────────────────────────────────────────────
      llama_swap:
        hostname: "llama-swap"
        container_name: "llama-swap"
        port: 8080                         # Internal container port
        published_port: 9292               # Published to host
        bind: "127.0.0.1"
        image: "ghcr.io/mostlygeek/llama-swap:cpu"
        external_subdomain: "llama-swap"
        websocket: true
        enabled: true
        volume: "llama-swap.volume"
        description: "Local LLM Model Router"


      whisper:
        hostname: "whisper"
        container_name: "whisper"
        port: 8000                          # Internal container port
        published_port: 8765                # Published to host (no conflicts)
        bind: "127.0.0.1"
        image: "docker.io/fedirz/faster-whisper-server:latest-cpu"
        external_subdomain: "whisper"       # Optional: external access via Caddy
        enabled_by: ["openwebui.features.speech_to_text"]  # Auto-enable when STT enabled
        websocket: false
        description: "Whisper Speech-to-Text Service"
        volume: "whisper-cache.volume"


      edgetts:
        hostname: "edgetts"
        container_name: "edgetts"
        port: 5050                          # Internal container port
        published_port: 5050                # Published to host
        bind: "127.0.0.1"
        image: "ghcr.io/traefik/parakeet:latest"  # Edge-TTS compatible server
        external_subdomain: "tts"           # Optional: external access via Caddy
        enabled_by: ["openwebui.features.text_to_speech"]
        websocket: false
        description: "Edge-TTS Text-to-Speech Service"
        volume: "edgetts-cache.volume"

      jupyter:
        hostname: "jupyter"
        container_name: "jupyter"
        port: 8888                          # Internal container port
        published_port: 8889                # Published to host (avoid SearXNG conflict)
        bind: "127.0.0.1"
        image: "localhost/blueprint-jupyter:latest"  # Custom built image
        external_subdomain: "jupyter"
        enabled_by: ["openwebui.features.code_execution"]
        requires: ["litellm"]               # Needs LiteLLM for notebook-intelligence
        websocket: true
        description: "Blueprint Jupyter Lab - AI Code Interpreter"
        volume: "jupyter.volume"
        workspace: "/home/jovyan/blueprint-workspace"
      
      # ─────────────────────────────────────────────────────────────────────────
      # System Management
      # ─────────────────────────────────────────────────────────────────────────
      cockpit:
        hostname: "{{ .tailscale.hostname }}"  # Runs on host
        container_name: "cockpit-ws"
        port: 9090
        published_port: 9090
        bind: "127.0.0.1"
        image: "quay.io/cockpit/ws:latest"
        external_subdomain: ""  # Root domain
        enabled: true
        websocket: true
        description: "System Management Dashboard"

  # ═══════════════════════════════════════════════════════════════════════════
  # 🔐 SECRETS REFERENCES (from encrypted_private_secrets.yaml)
  # ═══════════════════════════════════════════════════════════════════════════
  secrets:
    
    api_keys:
      litellm_master: "sk-litellm-local"
      openwebui_secret: "sk-openwebui-local"
      jupyter_token: "sk-jupyter-local"
    # Cloud LLM providers
    llm_providers:
      openai: "{{ .secrets.llm_providers.openai }}"
      anthropic: "{{ .secrets.llm_providers.anthropic }}"
      gemini: "{{ .secrets.llm_providers.gemini }}"
      groq:
      openrouter:
    
    # Optional: Search providers
    search_providers:
      tavily: "{{ .secrets.search_providers.tavily }}"
    
    # Optional: Audio providers
    audio_providers:
      elevenlabs: "{{ .secrets.audio_providers.elevenlabs }}"
      deepgram: "{{ .secrets.audio_providers.deepgram }}"


  # ═══════════════════════════════════════════════════════════════════════════
  # 🚀 LITELLM - LLM Proxy Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  litellm:
    # Database connection (auto-generated from infrastructure.services)
    database_url: "postgresql://litellm@{{ .infrastructure.services.litellm_postgres.hostname }}:{{ .infrastructure.services.litellm_postgres.port }}/litellm"
    
    # Redis cache connection (auto-generated)
    redis_url: "redis://{{ .infrastructure.services.litellm_redis.hostname }}:{{ .infrastructure.services.litellm_redis.port }}/0"
    
    # Master key for authentication
    master_key: "{{ .secrets.api_keys.litellm_master }}"
    
    # Models to expose
    models:

      # ───────────────────────────────────────────────────────────────────────
      # OPENAI GPT-5 FAMILY (Released August 7, 2025)
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "gpt-5"
        display_name: "GPT-5 (Flagship)"
        provider: "openai"
        full_model_id: "gpt-5-2025-08-07"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: true
        description: "OpenAI's flagship GPT-5 model - Advanced reasoning, multimodal processing, tool calling"
        capabilities:
          - "Advanced reasoning and problem-solving"
          - "Multimodal processing (text, images, audio)"
          - "Tool calling and function execution"
          - "Structured outputs (JSON, XML)"
          - "Long-form content generation"
        context_window: 400000  # 272K input + 128K output
        pricing:
          input: "$1.25/M tokens"
          output: "$10.00/M tokens"
        use_cases:
          - "Complex reasoning tasks"
          - "Multi-step problem solving"
          - "Code generation and debugging"
          - "Content creation and editing"
          - "Research and analysis"
        release_date: "2025-08-07"
        notes: "Unprecedented capabilities in reasoning, coding, and agentic tasks"
      
      - name: "gpt-5-mini"
        display_name: "GPT-5 Mini"
        provider: "openai"
        full_model_id: "gpt-5-mini-2025-08-07"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: true
        description: "Cost-efficient GPT-5 variant for well-defined tasks"
        capabilities:
          - "Fast response times"
          - "Cost-effective processing"
          - "Well-defined task execution"
          - "Multimodal support"
          - "Tool calling"
        context_window: 400000
        pricing:
          input: "$0.25/M tokens"
          output: "$2.00/M tokens"
        use_cases:
          - "High-volume API calls"
          - "Real-time chat applications"
          - "Simple content generation"
          - "Code completion"
          - "Quick Q&A"
        release_date: "2025-08-07"
        notes: "80% cost reduction vs GPT-5 while maintaining strong performance"
      
      - name: "gpt-5-nano"
        display_name: "GPT-5 Nano"
        provider: "openai"
        full_model_id: "gpt-5-nano-2025-08-07"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: true
        description: "Fastest, most cost-efficient GPT-5 for everyday tasks"
        capabilities:
          - "Ultra-fast inference"
          - "Minimal latency"
          - "Basic reasoning"
          - "Simple text generation"
        context_window: 400000
        pricing:
          input: "$0.05/M tokens"
          output: "$0.40/M tokens"
        use_cases:
          - "High-throughput applications"
          - "Simple classification"
          - "Basic Q&A"
          - "Text summarization"
          - "Sentiment analysis"
        release_date: "2025-08-07"
        notes: "96% cost reduction vs GPT-5 - ideal for edge cases and batch processing"
      
      - name: "gpt-5-pro"
        display_name: "GPT-5 Pro"
        provider: "openai"
        full_model_id: "gpt-5-pro-2025-10-06"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: false  # Premium tier - enable when needed
        description: "Scaled test-time compute for challenging reasoning tasks"
        capabilities:
          - "Extended reasoning time"
          - "Complex problem decomposition"
          - "Multi-step validation"
          - "Self-correction"
          - "Advanced logical reasoning"
        context_window: 400000
        pricing:
          input: "Premium tier"
          output: "Premium tier"
        use_cases:
          - "Mathematical proofs"
          - "Complex code refactoring"
          - "Research paper analysis"
          - "Strategic planning"
          - "Advanced debugging"
        release_date: "2025-10-06"
        notes: "Preferred in 67.8% of expert evaluations over GPT-5 Thinking"
        performance:
          expert_preference: "67.8%"
          vs_model: "GPT-5 Thinking"
      
      # Legacy GPT-4 models (for backward compatibility)
      - name: "gpt-4o"
        display_name: "GPT-4o (Legacy)"
        provider: "openai"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: false  # Deprecated - use GPT-5 family
        description: "Legacy GPT-4 Omni model - superseded by GPT-5"
        notes: "Maintained for backward compatibility only"
      
      - name: "gpt-4o-mini"
        display_name: "GPT-4o Mini (Legacy)"
        provider: "openai"
        api_key: "{{ .secrets.llm_providers.openai }}"
        enabled: false  # Deprecated - use GPT-5 Mini
        description: "Legacy GPT-4o Mini - superseded by GPT-5 Mini"
        notes: "Maintained for backward compatibility only"
      
      # ───────────────────────────────────────────────────────────────────────
      # ANTHROPIC CLAUDE 4.x FAMILY
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "claude-sonnet-4-5"
        display_name: "Claude Sonnet 4.5"
        provider: "anthropic"
        full_model_id: "claude-sonnet-4-5-20250929"
        api_key: "{{ .secrets.llm_providers.anthropic }}"
        enabled: true
        description: "Anthropic's most advanced Sonnet model - Hybrid reasoning with 30+ hour autonomy"
        capabilities:
          - "Hybrid reasoning (fast + deep thinking)"
          - "Enhanced tool orchestration"
          - "30+ hours autonomous operation"
          - "Extended context understanding"
          - "Advanced agentic workflows"
        context_window: 200000
        max_output: 64000
        pricing:
          input: "$3.00/M tokens"
          output: "$15.00/M tokens"
        use_cases:
          - "Long-running autonomous tasks"
          - "Complex multi-step workflows"
          - "Advanced code generation"
          - "Research and analysis"
          - "Content creation at scale"
        release_date: "2025-09-29"
        notes: "Most intelligent and efficient Sonnet model - efficient for everyday use"
        features:
          - "Hybrid reasoning mode"
          - "30+ hour memory span"
          - "Enhanced tool use"
          - "Improved code generation"
      
      - name: "claude-opus-4-1"
        display_name: "Claude Opus 4.1"
        provider: "anthropic"
        full_model_id: "claude-opus-4-1-20250805"
        api_key: "{{ .secrets.llm_providers.anthropic }}"
        enabled: true
        description: "Latest and most powerful Opus model - 7-hour memory span with extended thinking"
        capabilities:
          - "7-hour memory span"
          - "Improved agentic search"
          - "Extended thinking mode"
          - "Complex reasoning tasks"
          - "Advanced coding capabilities"
        context_window: 200000
        pricing:
          input: "Premium tier"
          output: "Premium tier"
        use_cases:
          - "Most challenging reasoning tasks"
          - "Complex software architecture"
          - "Advanced research"
          - "Strategic planning"
          - "Long-form content creation"
        release_date: "2025-08-05"
        notes: "Claude Opus 4.5 does NOT exist - Opus 4.1 is the latest Opus model"
        features:
          - "7-hour memory span"
          - "Extended thinking mode"
          - "Improved agentic search"
          - "Enhanced coding"
      
      # Legacy Claude 3.5 models (for backward compatibility)
      - name: "claude-3-5-sonnet"
        display_name: "Claude 3.5 Sonnet (Legacy)"
        provider: "anthropic"
        api_key: "{{ .secrets.llm_providers.anthropic }}"
        enabled: false  # Deprecated - use Claude Sonnet 4.5
        description: "Legacy Claude 3.5 Sonnet - superseded by Claude Sonnet 4.5"
        notes: "Maintained for backward compatibility only"
      
      - name: "claude-3-5-haiku"
        display_name: "Claude 3.5 Haiku (Legacy)"
        provider: "anthropic"
        api_key: "{{ .secrets.llm_providers.anthropic }}"
        enabled: false  # Deprecated
        description: "Legacy Claude 3.5 Haiku - superseded by Claude Sonnet 4.5"
        notes: "Maintained for backward compatibility only"
      
      # ───────────────────────────────────────────────────────────────────────
      # GOOGLE GEMINI 2.5 FAMILY
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "gemini-2.5-flash"
        display_name: "Gemini 2.5 Flash"
        provider: "gemini"
        api_key: "{{ .secrets.llm_providers.gemini }}"
        enabled: true
        description: "Google's fast multimodal model - 1M+ token context with reasoning control"
        capabilities:
          - "Ultra-long context (1M+ tokens)"
          - "Multimodal understanding"
          - "Configurable reasoning depth"
          - "Fast inference"
          - "Cost-effective processing"
        context_window: 1048576  # 1,048,576 tokens
        pricing:
          input: "$0.30/M tokens"
          output: "$2.50/M tokens"
        use_cases:
          - "Long document analysis"
          - "Video understanding"
          - "Code repository analysis"
          - "Multi-document synthesis"
          - "Real-time applications"
        release_date: "2025"
        notes: "Supports reasoning_effort parameter for controlling reasoning depth"
        parameters:
          reasoning_effort:
            - "low"
            - "medium"
            - "high"
      
      - name: "gemini-2.5-pro"
        display_name: "Gemini 2.5 Pro"
        provider: "gemini"
        api_key: "{{ .secrets.llm_providers.gemini }}"
        enabled: true
        description: "Google's most powerful model - 2M token context with thinking budget"
        capabilities:
          - "Extreme long context (2M tokens)"
          - "Advanced reasoning"
          - "Thinking budget configuration"
          - "Multi-step problem solving"
          - "Complex code generation"
        context_window: 2000000  # 2,000,000 tokens
        pricing:
          input: "Premium tier"
          output: "Premium tier"
        use_cases:
          - "Entire codebase analysis"
          - "Large-scale research"
          - "Multi-source synthesis"
          - "Complex reasoning tasks"
          - "Advanced code generation"
        release_date: "2025"
        notes: "Supports max_tokens_for_reasoning parameter for thinking budget control"
        parameters:
          reasoning_effort:
            - "low"
            - "medium"
            - "high"
          max_tokens_for_reasoning: "configurable"
      
      # Legacy Gemini 2.0 models (for backward compatibility)
      - name: "gemini-2.0-flash"
        display_name: "Gemini 2.0 Flash (Legacy)"
        provider: "gemini"
        api_key: "{{ .secrets.llm_providers.gemini }}"
        enabled: false  # Deprecated - use Gemini 2.5 Flash
        description: "Legacy Gemini 2.0 Flash - superseded by Gemini 2.5 Flash"
        notes: "Maintained for backward compatibility only"
      
      # ───────────────────────────────────────────────────────────────────────
      # X.AI GROK FAMILY (via OpenRouter)
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "grok-4-fast"
        display_name: "Grok 4 Fast"
        provider: "openrouter"
        openrouter_model: "x-ai/grok-4-fast:free"
        api_key: "{{ .secrets.llm_providers.openrouter }}"
        enabled: false  # Optional - currently FREE promotion
        description: "X.AI's fast reasoning model - 2M token context, currently FREE"
        capabilities:
          - "Reasoning and non-reasoning modes"
          - "Ultra-long context (2M tokens)"
          - "Fast inference"
          - "Cost-effective (currently FREE)"
        context_window: 2000000
        pricing:
          input: "FREE (limited time)"
          output: "FREE (limited time)"
        use_cases:
          - "Experimental workflows"
          - "High-volume testing"
          - "Cost-sensitive applications"
        notes: "Supports reasoning_enabled parameter. FREE promotion may be limited time"
        parameters:
          reasoning_enabled: "boolean"
      
      - name: "grok-code-fast-1"
        display_name: "Grok Code Fast 1"
        provider: "openrouter"
        openrouter_model: "x-ai/grok-code-fast-1"
        api_key: "{{ .secrets.llm_providers.openrouter }}"
        enabled: false  # Optional
        description: "X.AI's agentic coding model with visible reasoning traces"
        capabilities:
          - "Agentic code generation"
          - "Visible reasoning traces"
          - "Code debugging"
          - "Architecture planning"
        context_window: 256000
        pricing:
          input: "$0.20/M tokens"
          output: "$1.50/M tokens"
        use_cases:
          - "Code generation with explanations"
          - "Debugging with reasoning"
          - "Educational coding"
        notes: "Shows reasoning process - useful for learning and debugging"
      
      # ───────────────────────────────────────────────────────────────────────
      # DEEPSEEK V3.1 (via OpenRouter - FREE tier)
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "deepseek-chat-v3.1"
        display_name: "DeepSeek V3.1"
        provider: "openrouter"
        openrouter_model: "deepseek/deepseek-chat-v3.1:free"
        api_key: "{{ .secrets.llm_providers.openrouter }}"
        enabled: false  # Optional - FREE tier available
        description: "DeepSeek's hybrid reasoning model - 671B params, 37B active, FREE tier"
        capabilities:
          - "Hybrid reasoning (Think/Non-Think modes)"
          - "Mixture of Experts (MoE)"
          - "Cost-effective reasoning"
        context_window: 128000
        pricing:
          input: "FREE"
          output: "FREE"
        architecture:
          total_params: "671B"
          active_params: "37B"
        use_cases:
          - "Budget-conscious reasoning"
          - "Experimental workflows"
          - "High-volume applications"
        notes: "Supports reasoning_enabled boolean parameter"
        parameters:
          reasoning_enabled: "boolean"
      
      # ───────────────────────────────────────────────────────────────────────
      # ZHIPU AI GLM-4.6 (via OpenRouter)
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "glm-4.6"
        display_name: "GLM-4.6"
        provider: "openrouter"
        openrouter_model: "z-ai/glm-4.6"
        api_key: "{{ .secrets.llm_providers.openrouter }}"
        enabled: false  # Optional
        description: "Zhipu AI's real-world coding specialist - near parity with Claude Sonnet 4"
        capabilities:
          - "Real-world coding"
          - "Long-context processing"
          - "Advanced reasoning"
          - "RAG optimization"
        context_window: 200000
        max_output: 128000
        pricing:
          input: "Varies by provider"
          output: "Varies by provider"
        use_cases:
          - "Production code generation"
          - "Long document analysis"
          - "RAG applications"
        notes: "Near parity with Claude Sonnet 4 (48.6% win rate)"
        performance:
          claude_sonnet_4_parity: "48.6%"
      
      # ───────────────────────────────────────────────────────────────────────
      # ALIBABA QWEN3-MAX (via OpenRouter)
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "qwen3-max"
        display_name: "Qwen3-Max"
        provider: "openrouter"
        openrouter_model: "qwen/qwen3-max"
        api_key: "{{ .secrets.llm_providers.openrouter }}"
        enabled: false  # Optional
        description: "Alibaba's flagship model - 100+ languages, RAG-optimized"
        capabilities:
          - "100+ language support"
          - "RAG optimization"
          - "Tool calling"
          - "Long-context understanding"
        context_window: 256000
        pricing:
          input: "$1.20/M tokens"
          output: "$6.00/M tokens"
        use_cases:
          - "Multilingual applications"
          - "RAG pipelines"
          - "International projects"
        notes: "Excellent for non-English and RAG applications"
      
      # ───────────────────────────────────────────────────────────────────────
      # META LLAMA 3.3 (via OpenRouter - FREE tier)
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "llama-3.3-8b"
        display_name: "Llama 3.3 8B Instruct"
        provider: "openrouter"
        openrouter_model: "meta-llama/llama-3.3-8b-instruct:free"
        api_key: "{{ .secrets.llm_providers.openrouter }}"
        enabled: false  # Optional - FREE tier
        description: "Meta's lightweight model - 8B params, FREE tier, 128K context"
        capabilities:
          - "Fast inference"
          - "Cost-effective"
          - "Long context (128K)"
        context_window: 128000
        pricing:
          input: "FREE"
          output: "FREE"
        architecture:
          params: "8B"
        use_cases:
          - "High-volume applications"
          - "Budget-conscious projects"
          - "Experimental workflows"
        notes: "Lightweight variant of Llama 3.3 70B"
      
      # ───────────────────────────────────────────────────────────────────────
      # GROQ INFERENCE (OpenAI GPT-OSS via Groq infrastructure)
      # ───────────────────────────────────────────────────────────────────────
      
      - name: "gpt-oss-120b-groq"
        display_name: "GPT-OSS 120B (Groq)"
        provider: "groq"
        groq_model: "openai/gpt-oss-120b"
        api_key: "{{ .secrets.llm_providers.groq }}"
        enabled: false  # Optional - for ultra-fast inference
        description: "OpenAI's open-weight 120B model on Groq's infrastructure - 500 tokens/sec"
        capabilities:
          - "Ultra-fast inference (500 tok/s)"
          - "MoE architecture"
          - "Near o4-mini parity"
          - "Apache 2.0 license"
        context_window: 128000
        pricing:
          input: "$0.15/M tokens"
          output: "$0.75/M tokens"
        architecture:
          total_params: "120B"
          active_params: "5.1B per token"
          license: "Apache 2.0"
        use_cases:
          - "Real-time applications"
          - "Low-latency requirements"
          - "High-throughput processing"
        notes: "Groq's LPU infrastructure enables 500 tokens/second throughput"
        performance:
          speed: "500 tokens/second"
          vs_model: "Near-parity with o4-mini"

      # ───────────────────────────────────────────────────────────────────────
      # LOCAL MODELS (via llama-swap → ramalama)
      # ───────────────────────────────────────────────────────────────────────
      - name: "gpt-oss-20b"
        provider: "llama-swap"
        enabled: true
        endpoint: "http://{{ .infrastructure.services.llama_swap.hostname }}:{{ .infrastructure.services.llama_swap.port }}"
        description: "OpenAI GPT-OSS 20B - Fast local model"
      
      - name: "gpt-oss-120b"
        provider: "llama-swap"
        enabled: true
        endpoint: "http://{{ .infrastructure.services.llama_swap.hostname }}:{{ .infrastructure.services.llama_swap.port }}"
        description: "OpenAI GPT-OSS 120B - Powerful local model"
    
    # General settings
    drop_params: true  # Essential for compatibility
    log_level: "INFO"

  # ═══════════════════════════════════════════════════════════════════════════
  # 🤖 LOCAL INFERENCE - RamaLama + llama-swap Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  local_inference:
    enabled: true
    
    # Default settings for all local models
    defaults:
      runtime: "llama.cpp"
      backend: "vulkan"              # AMD GPU acceleration
      ctx_size: 8192                 # Context window
      ngl: 999                       # GPU layers (all)
      temp: 0.7                      # Temperature
      threads: 32                    # CPU threads (adjust for your system)
      cache_reuse: 256               # KV cache reuse for better performance
      auto_unload: true              # Enable TTL-based unloading
      container_image: "quay.io/ramalama/ramalama:latest"
      health_check_timeout: 3600     # Allow time for model downloads
      log_level: "info"
      metrics_max_in_memory: 1000
      start_port: 10001

    # Model groups
    groups:
      heavy:
        swap: true
        description: "Primary large models (one active at a time)"
        members:
          - "gpt-oss-20b"
          - "gpt-oss-120b"
      
      task:
        swap: false
        description: "Always-loaded task models (run in parallel, never swap)"
        members:
          - "qwen3-0.6b"
          - "qwen3-4b"

      embeddings:
        swap: false
        description: "Embedding models for RAG (always loaded, parallel execution)"
        members:
          - "qwen3-embedding-8b"

    # Model definitions
    models:
      gpt_oss_20b:
        name: "gpt-oss-20b"
        display_name: "GPT-OSS 20B"
        description: "OpenAI's 20B parameter model - Fast general tasks"
        hf_repo: "openai/gpt-oss-20b-GGUF"
        hf_file: "gpt-oss-20b-q4_k_m.gguf"
        model_uri: "huggingface://openai/gpt-oss-20b-GGUF/gpt-oss-20b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 16
        context_length: 8192
        ctx_size: 8192
        ttl: 600
        group: "heavy"
        vulkan_driver: "RADV"
        flash_attn: true
        aliases:
          - "gpt-oss"
        enabled: true
      
      llama_4_scout_17b:
        name: "llama-4-scout-17b"
        display_name: "Llama 4 Scout 17B"
        description: "Extended context specialist - 10M token capability"
        model_uri: "huggingface://unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF/Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 58
        context_length: 10485760      # 10M tokens!
        ctx_size: 65536                # Practical limit for speed
        ttl: 900
        vulkan_driver: "AMDVLK"        # Better for long prompts
        flash_attn: true
        # Note: Can theoretically do 1M context in 128GB, but extremely slow
        # Practical: 65k context is much more usable
        enabled: false                 # Enable when you need long context
      
      qwen3_coder_30b:
        name: "qwen3-coder-30b"
        display_name: "Qwen3 Coder 30B"
        description: "Advanced code model - great for complex refactoring"
        model_uri: "huggingface://unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-BF16.gguf"
        quantization: "BF16"
        ram_required_gb: 60
        context_length: 8192
        ctx_size: 8192
        ttl: 900
        vulkan_driver: "RADV"
        flash_attn: true
        enabled: false                 # Enable when you need serious coding help
      
      qwen3_thinking_30b:
        name: "qwen3-thinking-30b"
        display_name: "Qwen3 Thinking 30B"
        description: "Deep reasoning specialist - thinks through problems"
        model_uri: "huggingface://unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF/Qwen3-30B-A3B-Thinking-2507-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 20
        context_length: 8192
        ctx_size: 8192
        ttl: 900
        vulkan_driver: "AMDVLK"        # Better for long reasoning prompts
        flash_attn: true
        aliases:
          - "thinking"
          - "qwen-thinking"
        enabled: false
      
      qwq_32b:
        name: "qwq-32b"
        display_name: "QwQ 32B"
        description: "Question-answering specialist with reasoning"
        model_uri: "huggingface://bartowski/Qwen_QwQ-32B-GGUF/Qwen_QwQ-32B-Q4_K_M.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 20
        context_length: 32768
        ctx_size: 32768
        ttl: 900
        vulkan_driver: "AMDVLK"
        flash_attn: true
        enabled: false
      
      gpt_oss_120b:
        name: "gpt-oss-120b"
        display_name: "GPT-OSS 120B"
        description: "OpenAI's 120B parameter model - Complex reasoning"
        hf_repo: "openai/gpt-oss-120b-GGUF"
        hf_file: "gpt-oss-120b-q4_k_m.gguf"
        model_uri: "huggingface://openai/gpt-oss-120b-GGUF/gpt-oss-120b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 80
        context_length: 8192
        ctx_size: 8192
        ttl: 900
        group: "heavy"
        vulkan_driver: "AMDVLK"        # Better for long prompts
        flash_attn: true
        enabled: true
      
      qwen3_235b:
        name: "qwen3-235b"
        display_name: "Qwen3 235B"
        description: "Flagship model - incredible capability, fits in 128GB"
        model_uri: "huggingface://unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf"
        quantization: "Q3_K_XL"
        ram_required_gb: 97
        context_length: 131072
        ctx_size: 65536                # Conservative - can do 130k max
        ttl: 1800                      # 30 minutes
        vulkan_driver: "AMDVLK"        # Better for complex prompts
        flash_attn: true
        batch_size: 2048
        # Note: This is HUGE but fits in 128GB with Q3_K quantization
        # Expect slower inference but incredible quality
        enabled: false                 # Enable when you need the best

      qwen3_0_6b:
        name: "qwen3-0.6b"
        display_name: "Qwen3 0.6B"
        description: "Ultra-fast task model - title generation"
        model_uri: "huggingface://Qwen/Qwen3-0.6B-GGUF/qwen3-0.6b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 1
        context_length: 32768
        ctx_size: 4096
        ttl: 0  # Never unload
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-tiny"
        enabled: true
      
      qwen3_4b:
        name: "qwen3-4b"
        display_name: "Qwen3 4B"
        description: "Balanced task model - tags, queries, analysis"
        model_uri: "huggingface://Qwen/Qwen3-4B-GGUF/qwen3-4b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 3
        context_length: 32768
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-medium"
        enabled: true
      
      # Optional models for manual experimentation
      qwen3_1_7b:
        name: "qwen3-1.7b"
        display_name: "Qwen3 1.7B"
        description: "Fast task model - metadata tagging, summarization"
        model_uri: "huggingface://Qwen/Qwen3-1.7B-GGUF/qwen3-1.7b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 2
        context_length: 32768
        ctx_size: 8192
        ttl: 600  # Auto-unload after 10 min
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-small"
        enabled: false  # Disabled by default
      
      qwen3_8b:
        name: "qwen3-8b"
        display_name: "Qwen3 8B"
        description: "High-quality task model - complex reasoning"
        model_uri: "huggingface://Qwen/Qwen3-8B-GGUF/qwen3-8b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 6
        context_length: 32768
        ctx_size: 8192
        ttl: 600  # Auto-unload after 10 min
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-large"
        enabled: false  # Disabled by default
      
      qwen3_14b:
        name: "qwen3-14b"
        display_name: "Qwen3 14B"
        description: "Premium task model - advanced reasoning"
        model_uri: "huggingface://Qwen/Qwen3-14B-GGUF/qwen3-14b-q4_k_m.gguf"
        quantization: "Q4_K_M"
        ram_required_gb: 10
        context_length: 32768
        ctx_size: 8192
        ttl: 600  # Auto-unload after 10 min
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "qwen3-xlarge"
        enabled: false  # Disabled by default

      # ═══════════════════════════════════════════════════════════════════════
      # IBM GRANITE 4.0 MODELS
      # ═══════════════════════════════════════════════════════════════════════
      
      granite_4_0_h_micro:
        name: "granite-4.0-h-micro"
        display_name: "Granite 4.0 H-Micro"
        description: "IBM's 3B enterprise model - Fast, efficient for edge use"
        model_uri: "huggingface://unsloth/granite-4.0-h-micro-GGUF/granite-4.0-h-micro-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 3
        context_length: 131072
        ctx_size: 8192
        ttl: 600
        group: "task"
        vulkan_driver: "RADV"
        flash_attn: true
        shortname: "granite-micro"
        enabled: false
        aliases:
          - "granite-micro"
          - "granite-3b"
      
      granite_4_0_h_small:
        name: "granite-4.0-h-small"
        display_name: "Granite 4.0 H-Small"
        description: "IBM's 32B MoE model (9B active) - Enterprise workhorse"
        model_uri: "huggingface://unsloth/granite-4.0-h-small-GGUF/granite-4.0-h-small-UD-Q4_K_XL.gguf"
        quantization: "Q4_K_XL"
        ram_required_gb: 22
        context_length: 131072
        ctx_size: 16384
        ttl: 900
        group: "heavy"
        vulkan_driver: "AMDVLK"
        flash_attn: true
        shortname: "granite-small"
        enabled: false
        aliases:
          - "granite-small"
          - "granite-32b"

      # ═══════════════════════════════════════════════════════════════════════
      # EMBEDDING MODELS (for RAG/Vector Search)
      # ═══════════════════════════════════════════════════════════════════════
      
      qwen3_embedding_0_6b:
        name: "qwen3-embedding-0.6b"
        display_name: "Qwen3 Embedding 0.6B"
        description: "Ultra-fast embedding model - 0.6B parameters"
        model_uri: "huggingface://Qwen/Qwen3-Embedding-0.6B-GGUF/qwen3-embedding-0.6b-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 1
        context_length: 8192
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false  # Not applicable for embeddings
        embedding_dimension: 1024
        shortname: "qwen3-emb-tiny"
        enabled: false
      
      qwen3_embedding_4b:
        name: "qwen3-embedding-4b"
        display_name: "Qwen3 Embedding 4B"
        description: "Balanced embedding model - 4B parameters"
        model_uri: "huggingface://Qwen/Qwen3-Embedding-4B-GGUF/qwen3-embedding-4b-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 5
        context_length: 8192
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        embedding_dimension: 2048
        shortname: "qwen3-emb-medium"
        enabled: false
      
      qwen3_embedding_8b:
        name: "qwen3-embedding-8b"
        display_name: "Qwen3 Embedding 8B"
        description: "High-quality embedding model - 8B parameters (DEFAULT)"
        model_uri: "huggingface://Qwen/Qwen3-Embedding-8B-GGUF/qwen3-embedding-8b-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 9
        context_length: 8192
        ctx_size: 8192
        ttl: 0  # Never unload
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        embedding_dimension: 4096
        shortname: "qwen3-emb-large"
        enabled: true  # ✅ DEFAULT ENABLED
        aliases:
          - "qwen-embed"
          - "default-embedding"

      nomic_embed_text_v1_5:
        name: "nomic-embed-text-v1.5"
        display_name: "Nomic Embed Text v1.5"
        description: "OpenAI-compatible embedding (text-embedding-3-small alternative)"
        model_uri: "huggingface://nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 1
        context_length: 8192
        ctx_size: 8192
        ttl: 0
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        enabled: false
        aliases:
          - "text-embedding-3-small"
      
      voyage_3_large:
        name: "voyage-3-large"
        display_name: "Voyage 3 Large"
        description: "High-capacity embedding - 10B parameters"
        model_uri: "huggingface://voyageai/voyage-3-large-GGUF/voyage-3-large-q8_0.gguf"
        quantization: "Q8_0"
        ram_required_gb: 11
        context_length: 8192
        ctx_size: 8192
        ttl: 0
        group: "embeddings"
        vulkan_driver: "RADV"
        flash_attn: false
        enabled: false
        aliases:
          - "text-embedding-3-large"

  # ═══════════════════════════════════════════════════════════════════════════
  # 🎤 local whisper section
  # ═══════════════════════════════════════════════════════════════════════════
  whisper:
    # Model cache directory - shared with host CLI tools
    cache_dir: "{{ .chezmoi.homeDir }}/.cache/huggingface"
    
    # Model selection
    # Options: tiny (75MB), base (142MB), small (466MB), medium (1.5GB), large-v3 (2.9GB)
    model: "large-v3"
    
    # Language configuration
    language: "en"             # Specific language, or "" for auto-detect (slower)
    
    # Inference settings
    beam_size: 5               # Higher = more accurate but slower (1-10)
    compute_type: "auto"       # auto, int8, float16, float32
    device: "auto"             # auto detects Vulkan GPU
    
    # Performance notes for your AMD Strix Halo:
    # - base model: ~16x real-time (6sec audio in 0.375sec)
    # - small model: ~6x real-time
    # - large-v3: ~1x real-time

  # ═══════════════════════════════════════════════════════════════════════════
  # 🔊 Edge-TTS Configuration (TTS)
  # Add after whisper section
  # ═══════════════════════════════════════════════════════════════════════════
  edgetts:
    # Audio format
    response_format: "mp3"     # Options: mp3, opus, aac, flac, wav, pcm
    
    # Speech speed
    speed: "1.0"               # 0.5 = half speed, 2.0 = double speed
    
    # Default voice
    # Full list: https://speech.microsoft.com/portal/voicegallery
    voice: "en-AU-NatashaNeural"
    
    # Alternative voices to try:
    # English (US): en-US-AriaNeural, en-US-GuyNeural, en-US-JennyNeural
    # English (UK): en-GB-SoniaNeural, en-GB-RyanNeural
    # English (AU): en-AU-NatashaNeural, en-AU-WilliamNeural
    
    # Logging
    log_level: "info"          # debug, info, warning, error
    
    # Available voices by model (OpenAI-compatible mapping)
    models:
      tts-1:
        alloy: "en-US-AriaNeural"
        echo: "en-US-GuyNeural"
        fable: "en-GB-RyanNeural"
        onyx: "en-US-DavisNeural"
        nova: "en-US-JennyNeural"
        shimmer: "en-US-AmberNeural"
      
      tts-1-hd:
        alloy: "en-US-AriaNeural"
        echo: "en-US-GuyNeural"
        fable: "en-GB-RyanNeural"
        onyx: "en-US-DavisNeural"
        nova: "en-US-JennyNeural"
        shimmer: "en-US-AmberNeural"
       
  # ═══════════════════════════════════════════════════════════════════════════
  # 🤖 OPENWEBUI - Feature-Centric Configuration (29 Decision Variables)
  # ═══════════════════════════════════════════════════════════════════════════
  openwebui:
    # ─────────────────────────────────────────────────────────────────────────
    # 🎯 LAYER 1: Feature Flags (What capabilities to enable?)
    # ─────────────────────────────────────────────────────────────────────────
    features:
      # Core AI capabilities
      rag: true                           # 🔑 Decision 1: Enable RAG?
      web_search: true                    # 🔑 Decision 2: Enable web search?
      image_generation: false             # 🔑 Decision 3: Enable image gen?
      speech_to_text: true               # 🔑 Decision 4: Enable STT?
      text_to_speech: true               # 🔑 Decision 5: Enable TTS?
      
      # Code execution
      code_execution: true                # 🔑 Decision 6: Enable code execution?
      code_interpreter: true              # 🔑 Decision 7: Enable code interpreter?
      
      # Cloud integrations
      google_drive: false                 # 🔑 Decision 8: Enable Google Drive?
      onedrive: false                     # 🔑 Decision 9: Enable OneDrive?
      
      # Authentication
      oauth_signup: false                 # 🔑 Decision 10: Enable OAuth?
      ldap: false                         # 🔑 Decision 11: Enable LDAP?
      
      # Advanced features
      title_generation: false             # 🔑 Decision 12: Auto-generate titles?
      autocomplete_generation: false      # 🔑 Decision 13: Enable autocomplete?
      tags_generation: false              # 🔑 Decision 14: Auto-generate tags?
      
      # Infrastructure
      websocket_support: true             # 🔑 Decision 15: Enable WebSocket?
      direct_connections: true            # 🔑 Decision 16: Direct model connections?
    
    # ─────────────────────────────────────────────────────────────────────────
    # 🔌 LAYER 2: Provider Selections (Which implementation?)
    # ─────────────────────────────────────────────────────────────────────────
    providers:
      # Vector database (controls 48+ variables)
      vector_db: "pgvector"               # 🔑 Decision 17: chroma | pgvector | qdrant | milvus | opensearch | elasticsearch | pinecone
      
      # RAG components (controls 16+ variables)
      rag_embedding: "openai"                   # 🔑 Decision 18: "" (local) | ollama | openai
      content_extraction: "tika"          # 🔑 Decision 19: tika | docling | mistral_ocr | document_intelligence | external
      text_splitter: "character"          # 🔑 Decision 20: character | token
      
      # Web search (controls 50+ variables)
      web_search_engine: "searxng"        # 🔑 Decision 21: searxng | tavily | brave | google_pse | duckduckgo | kagi | etc
      web_loader: "requests"              # 🔑 Decision 22: requests | playwright | safe_web
      
      # Image generation (controls 12+ variables)
      image_engine: "openai"              # 🔑 Decision 23: openai | comfyui | automatic1111 | gemini
      
      # Audio (controls 20+ variables)
      stt_engine: "openai"                      # 🔑 Decision 24: "" (local whisper) | openai | azure | deepgram
      tts_engine: "openai"                # 🔑 Decision 25: openai | azure | elevenlabs | transformers
      
      # Code execution (controls 8+ variables)
      code_execution_engine: "jupyter"    # 🔑 Decision 26: jupyter | pyodide
      code_interpreter_engine: "jupyter"  # 🔑 Decision 27: jupyter | pyodide
      
      # Storage (controls 12+ variables)
      storage_provider: ""                # 🔑 Decision 28: "" (local) | s3 | gcs | azure
      
      # Authentication (controls 10+ variables)
      auth_provider: "local"              # 🔑 Decision 29: local | oauth | ldap
    
    # ─────────────────────────────────────────────────────────────────────────
    # ⚙️  LAYER 3: Provider Configurations (How to configure?)
    # Only the selected provider's config is used at runtime
    # ─────────────────────────────────────────────────────────────────────────
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ VECTOR DATABASE CONFIGS (Decision 17)                               │
    # └─────────────────────────────────────────────────────────────────────┘
    vector_db_config:
      pgvector:
        db_url: "postgresql://openwebui@{{ .infrastructure.services.openwebui_postgres.hostname }}:{{ .infrastructure.services.openwebui_postgres.port }}/openwebui"
        vector_dimension: 384

      chroma:
        # Embedded - no external service needed
        tenant: "default_tenant"
        database: "default_database"
      
      qdrant:
        # Auto-generated from infrastructure.services
        uri: "http://{{ .infrastructure.services.qdrant.hostname }}:{{ .infrastructure.services.qdrant.port }}"
        grpc_port: "{{ .infrastructure.services.qdrant.grpc_port }}"
        api_key: ""  # Optional
        prefer_grpc: false
        on_disk: true
      
      milvus:
        uri: "http://milvus:19530"
        token: ""
        index_type: "HNSW"
        metric_type: "COSINE"
      
      elasticsearch:
        url: "http://elasticsearch:9200"
        api_key: ""
        index_prefix: "openwebui"
      
      opensearch:
        uri: "http://opensearch:9200"
        username: "admin"
        password: ""
      
      pinecone:
        api_key: ""
        environment: "us-east-1"
        index_name: "openwebui"
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ RAG EMBEDDING CONFIGS (Decision 18)                                 │
    # └─────────────────────────────────────────────────────────────────────┘
    rag_embedding_config:
      local:
        model: "sentence-transformers/all-MiniLM-L6-v2"
        device: "cpu"
        batch_size: 1
      
      ollama:
        # Used when rag_embedding: "ollama" (native Ollama daemon)
        base_url: "http://ollama:11434"
        model: "nomic-embed-text:latest"

      openai:
        base_url: "http://{{ .infrastructure.services.litellm.hostname }}:{{ .infrastructure.services.litellm.port }}/v1"
        api_key: "{{ .secrets.api_keys.litellm_master }}"
        model: "qwen3-embedding-8b"  # ← DEFAULT embedding model

    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ CONTENT EXTRACTION CONFIGS (Decision 19)                            │
    # └─────────────────────────────────────────────────────────────────────┘
    content_extraction_config:
      tika:
        # Auto-generated from infrastructure.services
        server_url: "http://{{ .infrastructure.services.tika.hostname }}:{{ .infrastructure.services.tika.port }}"
      
      docling:
        # Auto-generated from infrastructure.services
        server_url: "http://{{ .infrastructure.services.docling.hostname }}:{{ .infrastructure.services.docling.port }}"
        ocr_engine: "tesseract"
        ocr_lang: "eng"
      
      mistral_ocr:
        api_key: "{{ .secrets.llm_providers.mistral }}"
      
      external:
        url: ""
        api_key: ""
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ WEB SEARCH CONFIGS (Decision 21)                                    │
    # └─────────────────────────────────────────────────────────────────────┘
    web_search_config:
      searxng:
        # Auto-generated from infrastructure.services
        query_url: "http://{{ .infrastructure.services.searxng.hostname }}:{{ .infrastructure.services.searxng.port }}/search?q=<query>&format=json"
      
      tavily:
        api_key: "{{ .secrets.search_providers.tavily }}"
        extract_depth: "basic"
      
      brave:
        api_key: "{{ .secrets.search_providers.brave }}"
      
      #perplexity:
      #  api_key: "{{ .secrets.search_providers.preplexity}}"
    
      google_pse:
        api_key: ""
        engine_id: ""
      
      duckduckgo: {}
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ WEB LOADER CONFIGS (Decision 22)                                    │
    # └─────────────────────────────────────────────────────────────────────┘
    web_loader_config:
      requests: {}
      
      playwright:
        ws_url: "ws://playwright:3000"
        timeout: 10000
      
      safe_web: {}
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ IMAGE GENERATION CONFIGS (Decision 23)                              │
    # └─────────────────────────────────────────────────────────────────────┘
    image_generation_config:
      openai:
        # Auto-generated - uses LiteLLM proxy
        base_url: "http://{{ .infrastructure.services.litellm.hostname }}:{{ .infrastructure.services.litellm.port }}/v1"
        api_key: "{{ .secrets.api_keys.litellm_master }}"
        model: "dall-e-3"
      
      comfyui:
        base_url: "http://comfyui:8188"
        workflow: ""
      
      automatic1111:
        base_url: "http://automatic1111:7860"
        api_auth: ""
      
      gemini:
        api_key: "{{ .secrets.llm_providers.gemini }}"
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ AUDIO CONFIGS (Decisions 24-25)                                     │
    # └─────────────────────────────────────────────────────────────────────┘
    audio_config:
      stt:
        openai:
        # Auto-generated - points to local whisper container
          base_url: "http://{{ .infrastructure.services.whisper.hostname }}:{{ .infrastructure.services.whisper.port }}/v1"
          api_key: "sk-whisper-local"  # Dummy key (not validated for local)
          model: "whisper-1"

        deepgram:
          api_key: "{{ .secrets.audio_providers.deepgram }}"
      
      tts:
        openai:
          # Auto-generated - points to edgetts container
          base_url: "http://{{ .infrastructure.services.edgetts.hostname }}:{{ .infrastructure.services.edgetts.port }}/v1"
          api_key: "sk-edgetts-local"
          model: "tts-1"
          voice: "{{ .edgetts.models.tts-1.alloy }}"  # Default to Aria
        
        elevenlabs:
          api_key: "{{ .secrets.audio_providers.elevenlabs }}"
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ CODE EXECUTION CONFIGS (Decisions 26-27)                            │
    # └─────────────────────────────────────────────────────────────────────┘
    code_execution_config:
      jupyter:
        # Auto-generated from infrastructure.services
        url: "http://{{ .infrastructure.services.jupyter.hostname }}:{{ .infrastructure.services.jupyter.port }}"
        auth: "none"
        auth: "none"
        token: ""
        timeout: 60
      
      pyodide: {}
    
    # ┌─────────────────────────────────────────────────────────────────────┐
    # │ STORAGE CONFIGS (Decision 28)                                       │
    # └─────────────────────────────────────────────────────────────────────┘
    storage_config:
      local: {}
      
      s3:
        endpoint_url: ""
        bucket_name: "openwebui-storage"
        region_name: "us-east-1"
        access_key_id: ""
        secret_access_key: ""
      
      gcs:
        bucket_name: "openwebui-storage"
        credentials_json: ""
      
      azure:
        endpoint: ""
        container_name: "openwebui-storage"
        key: ""
    
    # ─────────────────────────────────────────────────────────────────────────
    # 🎨 General Settings (Provider-agnostic)
    # ─────────────────────────────────────────────────────────────────────────
    general:
      # Core connection (auto-generated from infrastructure.services)
      openai_api_base_url: "http://{{ .infrastructure.services.litellm.hostname }}:{{ .infrastructure.services.litellm.port }}/v1"
      openai_api_key: "{{ .secrets.api_keys.litellm_master }}"
      webui_secret_key: "{{ .secrets.api_keys.openwebui_secret }}"
      
      # Branding
      webui_name: "Leger AI"
      custom_name: "Blueprint LLM Stack"
      
      # Authentication
      webui_auth: false
      enable_signup: false
      default_user_role: "user"
      
      log_level: "info"  # debug | info | warning | error
      
      # Performance
      enable_realtime_chat_save: false
      
      # RAG settings
      rag_top_k: 3
      chunk_size: 1500
      chunk_overlap: 100
      pdf_extract_images: true
      
      # File upload limits
      rag_file_max_size: 50
      rag_file_max_count: 10
      
      enable_search_query: true
      enable_retrieval_query_generation: false
      
      # Web search settings
      web_search_result_count: 3
      web_search_concurrent_requests: 10
      
      enable_reranking: false
      reranking_model: "BAAI/bge-reranker-v2-m3"
      enable_hybrid_search: false
      enable_query_rewriting: false
      
      # ────────────────────────────────────────────────────────────────────────
      # 🤖 TASK MODEL ASSIGNMENTS (via LiteLLM)
      # ────────────────────────────────────────────────────────────────────────
      task_models:
        # Ultra-fast (0.6B) - single-task operations
        title_generation: "qwen3-0.6b"
        autocomplete_generation: "qwen3-0.6b"
        
        # Balanced (4B) - multi-step operations
        tags_generation: "qwen3-4b"
        query_generation: "qwen3-4b"
        search_query_generation: "qwen3-4b"
        rag_template_generation: "qwen3-4b"
    
    # ─────────────────────────────────────────────────────────────────────────
    # 🔧 Service Configuration
    # ─────────────────────────────────────────────────────────────────────────
    service:
      timeout_start_sec: 900
      restart: "on-failure"
      restart_sec: 10

  # ═══════════════════════════════════════════════════════════════════════════
  # 🎨 CADDY - Reverse Proxy (Auto-generated from infrastructure.services)
  # ═══════════════════════════════════════════════════════════════════════════
  caddy:
    # Caddy automatically configures routes based on infrastructure.services
    # where external_subdomain is defined
    auto_https: "off"  # Tailscale handles TLS
    
    # Optional: Manual route overrides
    custom_routes: []
  
  # ═══════════════════════════════════════════════════════════════════════════
  # 🔍 SEARXNG - Search Engine Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  searxng:
    # Redis connection (auto-generated from infrastructure.services)
    redis_url: "redis://{{ .infrastructure.services.searxng_redis.hostname }}:{{ .infrastructure.services.searxng_redis.port }}/0"
    
    # Base URL (auto-generated from Tailscale config)
    base_url: "https://{{ .infrastructure.services.searxng.external_subdomain }}.{{ .tailscale.full_hostname }}"
    
    # Settings
    safe_search: 0
    max_page: 5
    autocomplete: "duckduckgo"
    
    # Engines
    engines:
      - name: "duckduckgo"
        enabled: true
      - name: "google"
        enabled: true
      - name: "brave"
        enabled: false
      - name: "wikipedia"
        enabled: true
      - name: "github"
        enabled: true
