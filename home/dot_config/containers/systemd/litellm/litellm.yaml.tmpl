# ═════════════════════════════════════════════════════════════════════════════
# 🤖 LITELLM CONFIGURATION
# Auto-generated from .chezmoi.yaml.tmpl
# ═════════════════════════════════════════════════════════════════════════════

# ═════════════════════════════════════════════════════════════════════════════
# 🤖 MODEL DEFINITIONS
# ═════════════════════════════════════════════════════════════════════════════
model_list:
  # ───────────────────────────────────────────────────────────────────────────
  # CLOUD MODELS (OpenAI, Anthropic, Google)
  # Auto-generated from .chezmoi.yaml.tmpl → litellm.models
  # ───────────────────────────────────────────────────────────────────────────
  {{- range .litellm.models }}
  {{- if eq .provider "openai" }}
  
  # {{ .name }} (OpenAI)
  - model_name: {{ .name }}
    litellm_params:
      model: openai/{{ .name }}
      api_key: os.environ/OPENAI_API_KEY
  {{- else if eq .provider "anthropic" }}
  
  # {{ .name }} (Anthropic)
  - model_name: {{ .name }}
    litellm_params:
      model: anthropic/{{ .name }}
      api_key: os.environ/ANTHROPIC_API_KEY
  {{- else if eq .provider "gemini" }}
  
  # {{ .name }} (Google Gemini)
  - model_name: {{ .name }}
    litellm_params:
      model: gemini/{{ .name }}
      api_key: os.environ/GEMINI_API_KEY
  {{- end }}
  {{- end }}
  
  # ───────────────────────────────────────────────────────────────────────────
  # LOCAL MODELS (via llama-swap → ramalama)
  # Auto-generated from .chezmoi.yaml.tmpl → local_inference.models
  # ───────────────────────────────────────────────────────────────────────────
  {{- range $key, $model := .local_inference.models }}
  {{- if $model.enabled }}
  
  # {{ $model.display_name }} (Local)
  - model_name: {{ $model.name }}
    litellm_params:
      model: openai/{{ $model.name }}
      api_base: http://{{ $.infrastructure.services.llama_swap.hostname }}:{{ $.infrastructure.services.llama_swap.port }}/v1
      api_key: "sk-no-key-required"
      stream: true
      custom_llm_provider: openai
  {{- end }}
  {{- end }}

# ═════════════════════════════════════════════════════════════════════════════
# ⚙️  LITELLM SETTINGS
# ═════════════════════════════════════════════════════════════════════════════
litellm_settings:
  # Essential for compatibility with various providers
  drop_params: {{ .litellm.drop_params | default true }}
  
  # Redis caching (auto-configured from infrastructure.services)
  cache: true
  cache_params:
    type: redis
    host: {{ .infrastructure.services.litellm_redis.hostname }}
    port: {{ .infrastructure.services.litellm_redis.port }}
    ttl: 3600
  
  # Logging
  set_verbose: false

# ═════════════════════════════════════════════════════════════════════════════
# 🔧 GENERAL SETTINGS
# Database URL auto-generated from infrastructure.services
# ═════════════════════════════════════════════════════════════════════════════
general_settings:
  # Database connection (auto-generated)
  database_url: "postgresql://{{ .infrastructure.services.litellm_postgres.db_user }}@{{ .infrastructure.services.litellm_postgres.hostname }}:{{ .infrastructure.services.litellm_postgres.port }}/{{ .infrastructure.services.litellm_postgres.db_name }}"
  
  # Master key from secrets
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Store models in database
  store_model_in_db: true

# ═════════════════════════════════════════════════════════════════════════════
# 📝 NOTES
# ═════════════════════════════════════════════════════════════════════════════
#
# This file is auto-generated from .chezmoi.yaml.tmpl
#
# CLOUD MODELS are defined in: litellm.models
# LOCAL MODELS are defined in: local_inference.models
#
# LOCAL MODEL ROUTING:
#   OpenWebUI/Client → LiteLLM:4000 → llama-swap:9292 → ramalama containers
#
# When a request comes in for a local model (e.g. "{{ (index .local_inference.models 0).name }}"):
#   1. LiteLLM receives request at :4000/v1/chat/completions
#   2. Looks up model in model_list
#   3. Sees api_base points to llama-swap
#   4. Forwards to llama-swap:9292/v1/chat/completions
#   5. llama-swap checks if ramalama container is running
#   6. If not, spawns it via podman socket
#   7. Routes request to ramalama container
#   8. Returns response through the chain
#
# ADDING MORE MODELS:
#   1. Edit .chezmoi.yaml.tmpl (local_inference.models or litellm.models)
#   2. Run: chezmoi apply
#   3. Restart: systemctl --user restart llama-swap litellm
#
# ═════════════════════════════════════════════════════════════════════════════
