# ═════════════════════════════════════════════════════════════════════════════
# 🔄 LLAMA-SWAP CONFIGURATION
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# Auto-generated from .chezmoi.yaml.tmpl → local_inference.models
# ═════════════════════════════════════════════════════════════════════════════

# ═════════════════════════════════════════════════════════════════════════════
# ⚙️  GLOBAL SETTINGS
# ═════════════════════════════════════════════════════════════════════════════

# Health check timeout (seconds) - allow time for large model downloads
healthCheckTimeout: {{ .local_inference.defaults.health_check_timeout | default 3600 }}

# Log level
logLevel: {{ .local_inference.defaults.log_level | default "info" }}

# Metrics retention
metricsMaxInMemory: {{ .local_inference.defaults.metrics_max_in_memory | default 1000 }}

# Starting port for ${PORT} macro (auto-incremented per model)
startPort: {{ .local_inference.defaults.start_port | default 10001 }}

# ═════════════════════════════════════════════════════════════════════════════
# 🤖 MODEL DEFINITIONS
# Each model gets its own ramalama container, spawned on-demand
# Auto-generated from .chezmoi.yaml.tmpl → local_inference.models
# ═════════════════════════════════════════════════════════════════════════════

models:
{{- range $key, $model := .local_inference.models }}
{{- if $model.enabled }}

  # ───────────────────────────────────────────────────────────────────────────
  # {{ $model.display_name }}
  # ───────────────────────────────────────────────────────────────────────────
  "{{ $model.name }}":
    # Display name
    name: "{{ $model.display_name }}"
    
    # Description
    description: "{{ $model.description }}"
    
    # Proxy URL - where llama-swap routes requests
    # Container name: ramalama-{{ $model.name }} (on llm.network)
    proxy: "http://ramalama-{{ $model.name }}:${PORT}"
    
    # TTL - Auto-unload after inactivity
    ttl: {{ $model.ttl }}
    
    # Command to spawn ramalama container via mounted podman socket
    cmd: >
      podman run -d
      --name ramalama-{{ $model.name }}
      --network {{ $.infrastructure.network.name }}
      --device /dev/dri
      --device /dev/kfd
      --security-opt label=disable
      -v {{ $.chezmoi.homeDir }}/.local/share/ramalama:/root/.local/share/ramalama:Z
      -e RAMALAMA_IN_CONTAINER=true
      {{ $.local_inference.defaults.container_image | default "quay.io/ramalama/ramalama:latest" }}
      serve
      --runtime {{ $model.runtime | default $.local_inference.defaults.runtime }}
      --host 0.0.0.0
      --port ${PORT}
      --ctx-size {{ $model.ctx_size }}
      --ngl {{ $model.ngl | default $.local_inference.defaults.ngl }}
      {{ $model.model_uri }}
    
    # Command to stop container gracefully
    cmdStop: >
      podman stop -t 10 ramalama-{{ $model.name }} &&
      podman rm ramalama-{{ $model.name }}
    
    # Health check endpoint
    checkEndpoint: {{ $model.health_endpoint | default "/health" }}
    
    {{- if $model.aliases }}
    # Aliases - allow calling this model by familiar names
    aliases:
    {{- range $model.aliases }}
      - "{{ . }}"
    {{- end }}
    {{- end }}
{{- end }}
{{- end }}

# ═════════════════════════════════════════════════════════════════════════════
# 📝 CONFIGURATION NOTES
# ═════════════════════════════════════════════════════════════════════════════
#
# This file is auto-generated from .chezmoi.yaml.tmpl
# To add/remove models, edit the local_inference.models section there
#
# MODEL PATHS:
# Define model_uri in .chezmoi.yaml.tmpl using:
#   - HuggingFace: huggingface://org/repo/file.gguf
#   - Ollama: ollama://model:tag
#   - File: file:///path/to/model.gguf
#
# AMD STRIX HALO OPTIMIZATION:
#   --device /dev/dri        → Direct Rendering Infrastructure
#   --device /dev/kfd        → Kernel Fusion Driver
#   --ngl 999                → Load all layers on GPU (Vulkan)
#   Unified memory: 128GB RAM = GPU accessible
#
# MANUAL MODEL OPERATIONS:
#   Pre-download models: ramalama pull <uri>
#   List models: ramalama list
#   Remove model: ramalama rm <model>
#
# ═════════════════════════════════════════════════════════════════════════════
