# ═════════════════════════════════════════════════════════════════════════════
# 🤖 RAMALAMA CONFIGURATION
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# Location: ~/.config/ramalama/ramalama.conf
# ═════════════════════════════════════════════════════════════════════════════

[ramalama]

# ═════════════════════════════════════════════════════════════════════════════
# 🐋 CONTAINER ENGINE
# ═════════════════════════════════════════════════════════════════════════════

# Use Podman for better security and rootless containers
engine = "podman"

# Run models in containers by default (managed by llama-swap)
container = true

# Keep user groups when running containers (needed for GPU access)
keep_groups = true

# ═════════════════════════════════════════════════════════════════════════════
# 💾 STORAGE
# ═════════════════════════════════════════════════════════════════════════════

# Model storage location (auto-mounted by llama-swap)
store = "{{ .chezmoi.homeDir }}/.local/share/ramalama"

# ═════════════════════════════════════════════════════════════════════════════
# 🔧 DEFAULT RUNTIME
# ═════════════════════════════════════════════════════════════════════════════

# Use llama.cpp for best AMD GPU support (Vulkan backend)
runtime = "{{ .local_inference.defaults.runtime }}"

# Default transport for pulling models
transport = "huggingface"

# ═════════════════════════════════════════════════════════════════════════════
# 🌐 SERVING DEFAULTS
# ═════════════════════════════════════════════════════════════════════════════

# Default host (llama-swap will override this)
host = "0.0.0.0"

# Default port (llama-swap will override this per model)
port = "8080"

# ═════════════════════════════════════════════════════════════════════════════
# 🚀 MODEL DEFAULTS (Auto-generated from .chezmoi.yaml.tmpl)
# ═════════════════════════════════════════════════════════════════════════════

# Context window size
ctx_size = {{ .local_inference.defaults.ctx_size }}

# Temperature (creativity vs determinism)
temp = "{{ .local_inference.defaults.temp }}"

# GPU layers (-1 = auto-detect, 999 = all layers)
# AMD Strix Halo: Use all available VRAM
ngl = {{ .local_inference.defaults.ngl }}

# CPU threads (adjust based on your system)
threads = {{ .local_inference.defaults.threads }}

# KV cache reuse (performance optimization)
cache_reuse = {{ .local_inference.defaults.cache_reuse }}

# Model pull policy
pull = "newer"

# ═════════════════════════════════════════════════════════════════════════════
# 🎨 CONTAINER IMAGE
# ═════════════════════════════════════════════════════════════════════════════

# RamaLama container image (includes llama.cpp with Vulkan support)
image = "{{ .local_inference.defaults.container_image }}"

# ═════════════════════════════════════════════════════════════════════════════
# 🎯 VULKAN BACKEND SELECTION
# ═════════════════════════════════════════════════════════════════════════════
# 
# Based on AMD Strix Halo benchmarks:
# - RADV: Best for token generation (interactive chat) - 10 first-place finishes
# - AMDVLK: Best for prompt processing (batch) - 6 first-place finishes
#
# Default: RADV (best for typical chat usage)
# Override per-model in llama-swap config for prompt-heavy workloads
#
# To override globally, uncomment:
# [ramalama.env]
# AMD_VULKAN_ICD = "RADV"

# ═════════════════════════════════════════════════════════════════════════════
# 📝 NOTES
# ═════════════════════════════════════════════════════════════════════════════
# 
# This file is auto-generated from .chezmoi.yaml.tmpl
# All defaults come from: local_inference.defaults
#
# AMD Strix Halo Unified Memory:
# - 128GB RAM is fully accessible by GPU via Vulkan
# - No explicit VRAM allocation needed
# - Models limited by context size, not VRAM
#
# Vulkan Performance (from benchmarks):
# - Token Generation (tg128): RADV wins 10/17 models
# - Prompt Processing (pp512): AMDVLK wins 6/17 models
# - Balanced: AMDVLK slightly ahead overall
#
# Flash Attention: +6-11% prompt processing speed
# - Enabled automatically in llama-swap configurations
#
# Manual model operations:
#   ramalama pull <model-uri>
#   ramalama list
#   ramalama inspect <model>
#   ramalama rm <model>
# 
# ═════════════════════════════════════════════════════════════════════════════
