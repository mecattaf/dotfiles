# ═════════════════════════════════════════════════════════════════════════════
# 🤖 RAMALAMA CONFIGURATION
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# Location: ~/.config/ramalama/ramalama.conf
# ═════════════════════════════════════════════════════════════════════════════

[ramalama]

# ═════════════════════════════════════════════════════════════════════════════
# 🐋 CONTAINER ENGINE
# ═════════════════════════════════════════════════════════════════════════════

# Use Podman for better security and rootless containers
engine = "podman"

# Run models in containers by default (managed by llama-swap)
container = true

# Keep user groups when running containers (needed for GPU access)
keep_groups = true

# ═════════════════════════════════════════════════════════════════════════════
# 💾 STORAGE
# ═════════════════════════════════════════════════════════════════════════════

# Model storage location (auto-mounted by llama-swap)
store = "{{ .chezmoi.homeDir }}/.local/share/ramalama"

# ═════════════════════════════════════════════════════════════════════════════
# 🔧 DEFAULT RUNTIME
# ═════════════════════════════════════════════════════════════════════════════

# Use llama.cpp for best AMD GPU support (Vulkan backend)
runtime = "llama.cpp"

# Default transport for pulling models
transport = "huggingface"
#transport = "ollama"

# ═════════════════════════════════════════════════════════════════════════════
# 🌐 SERVING DEFAULTS
# ═════════════════════════════════════════════════════════════════════════════

# Default host (llama-swap will override this)
host = "0.0.0.0"

# Default port (llama-swap will override this per model)
port = "8080"

# ═════════════════════════════════════════════════════════════════════════════
# 🚀 MODEL DEFAULTS
# ═════════════════════════════════════════════════════════════════════════════

# Context window size
# GPT-OSS-20B: 8k tokens
# GPT-OSS-120B: 8k tokens (extended from original 4k)
ctx_size = 8192
#ctx_size = 32768

# Temperature (creativity vs determinism)
temp = "0.9"

# GPU layers (-1 = auto-detect, 999 = all layers)
# AMD Strix Halo: Use all available VRAM
#ngl = 999
ngl = -1

# Model pull policy
pull = "newer"

# ═════════════════════════════════════════════════════════════════════════════
# 🎨 CONTAINER IMAGE
# ═════════════════════════════════════════════════════════════════════════════

# RamaLama container image (includes llama.cpp with Vulkan support)
image = "quay.io/ramalama/ramalama:latest"

# ═════════════════════════════════════════════════════════════════════════════
# 📝 NOTES
# ═════════════════════════════════════════════════════════════════════════════
# 
# This configuration is optimized for AMD Ryzen AI MAX+ (Strix Halo)
# - Vulkan backend for GPU acceleration
# - Unified memory architecture (128GB system RAM = GPU accessible)
# - Models managed by llama-swap (automatic loading/unloading)
# 
# Models configured:
#   - openai/gpt-oss-20b  (~12GB RAM, fast)
#   - openai/gpt-oss-120b (~70GB RAM, powerful)
# 
# Manual model operations:
#   ramalama pull huggingface://openai/gpt-oss-20b
#   ramalama pull huggingface://openai/gpt-oss-120b
#   ramalama list
#   ramalama rm <model>
# 
# ═════════════════════════════════════════════════════════════════════════════
# RamaLama Configuration File for GPT-OSS Models
# This configuration is designed to run OpenAI's gpt-oss-120b and gpt-oss-20b models
# Place this file at: ~/.config/ramalama/ramalama.conf
