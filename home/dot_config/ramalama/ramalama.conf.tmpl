# ═════════════════════════════════════════════════════════════════════════════
# 🤖 RAMALAMA CONFIGURATION
# Generated by Chezmoi - DO NOT EDIT MANUALLY
# Location: ~/.config/ramalama/ramalama.conf
# ═════════════════════════════════════════════════════════════════════════════

[ramalama]

# ═════════════════════════════════════════════════════════════════════════════
# 🐋 CONTAINER ENGINE
# ═════════════════════════════════════════════════════════════════════════════

# Use Podman for better security and rootless containers
engine = "podman"

# Run models in containers by default (managed by llama-swap)
container = true

# Keep user groups when running containers (needed for GPU access)
keep_groups = true

# ═════════════════════════════════════════════════════════════════════════════
# 💾 STORAGE
# ═════════════════════════════════════════════════════════════════════════════

# Model storage location (auto-mounted by llama-swap)
store = "{{ .chezmoi.homeDir }}/.local/share/ramalama"

# ═════════════════════════════════════════════════════════════════════════════
# 🔧 DEFAULT RUNTIME
# ═════════════════════════════════════════════════════════════════════════════

# Use llama.cpp for best AMD GPU support (Vulkan backend)
runtime = "llama.cpp"

# Default transport for pulling models
transport = "huggingface"
#transport = "ollama"

# ═════════════════════════════════════════════════════════════════════════════
# 🌐 SERVING DEFAULTS
# ═════════════════════════════════════════════════════════════════════════════

# Default host (llama-swap will override this)
host = "0.0.0.0"

# Default port (llama-swap will override this per model)
port = "8080"

# ═════════════════════════════════════════════════════════════════════════════
# 🚀 MODEL DEFAULTS
# ═════════════════════════════════════════════════════════════════════════════

# Context window size
# GPT-OSS-20B: 8k tokens
# GPT-OSS-120B: 8k tokens (extended from original 4k)
ctx_size = 8192
#ctx_size = 32768

# Temperature (creativity vs determinism)
temp = "0.9"

# GPU layers (-1 = auto-detect, 999 = all layers)
# AMD Strix Halo: Use all available VRAM
#ngl = 999
ngl = -1

# Model pull policy
pull = "newer"

# ═════════════════════════════════════════════════════════════════════════════
# 🎨 CONTAINER IMAGE
# ═════════════════════════════════════════════════════════════════════════════

# RamaLama container image (includes llama.cpp with Vulkan support)
image = "quay.io/ramalama/ramalama:latest"

# ═════════════════════════════════════════════════════════════════════════════
# 🎯 VULKAN BACKEND SELECTION
# ═════════════════════════════════════════════════════════════════════════════
# 
# Based on AMD Strix Halo benchmarks:
# - RADV: Best for token generation (interactive chat) - 10 first-place finishes
# - AMDVLK: Best for prompt processing (batch) - 6 first-place finishes
#
# Default: RADV (best for typical chat usage)
# Override per-model in llama-swap config for prompt-heavy workloads
#
# To override globally, uncomment:
# [ramalama.env]
# AMD_VULKAN_ICD = "RADV"

# ═════════════════════════════════════════════════════════════════════════════
# 📝 NOTES
# ═════════════════════════════════════════════════════════════════════════════
# 
# AMD Strix Halo Unified Memory:
# - 128GB RAM is fully accessible by GPU via Vulkan
# - No explicit VRAM allocation needed
# - Models limited by context size, not VRAM
#
# Vulkan Performance (from benchmarks):
# - Token Generation (tg128): RADV wins 10/17 models
# - Prompt Processing (pp512): AMDVLK wins 6/17 models
# - Balanced: AMDVLK slightly ahead overall
#
# Flash Attention: +6-11% prompt processing speed
# - Enabled automatically in llama-swap configurations
#
# Manual model operations:
#   ramalama pull <model-uri>
#   ramalama list
#   ramalama inspect <model>
#   ramalama rm <model>
# 
# ═════════════════════════════════════════════════════════════════════════════
# RamaLama Configuration File for GPT-OSS Models
# This configuration is designed to run OpenAI's gpt-oss-120b and gpt-oss-20b models
# Place this file at: ~/.config/ramalama/ramalama.conf
