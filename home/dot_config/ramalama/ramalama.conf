# RamaLama Configuration File for GPT-OSS Models
# This configuration is designed to run OpenAI's gpt-oss-120b and gpt-oss-20b models
# Place this file at: ~/.config/ramalama/ramalama.conf

[ramalama]
# Container engine - Podman is recommended for better security and rootless containers
engine = "podman"

# Default storage location for AI models
store = "$HOME/.local/share/ramalama"

# Default transport for pulling models - ollama is used for gpt-oss models
transport = "ollama"

# Default runtime - llama.cpp is recommended for these models
runtime = "llama.cpp"

# Default host for serving models
host = "0.0.0.0"

# Default port for API services
port = "8080"

# Context size - both gpt-oss models support up to 128k tokens
# Setting to 32k for good performance vs memory balance
ctx_size = 32768

# Temperature setting for model responses
# 0.8 provides good balance between creativity and determinism
temp = "0.8"

# GPU layers - -1 means automatically determine appropriate layers
# For gpt-oss-120b you'll want maximum GPU utilization if you have enough VRAM (80GB+)
# For gpt-oss-20b you can run on systems with 16GB+ RAM/VRAM
ngl = -1

# Image pull policy - newer ensures you get updated model versions
pull = "newer"

# Keep groups setting for GPU access in rootless containers
keep_groups = true

# Container image for RamaLama
image = "quay.io/ramalama/ramalama:latest"

# Run in container by default for security
container = true
